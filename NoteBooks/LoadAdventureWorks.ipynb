{"cells":[{"cell_type":"code","source":["#V order thing you can ignore those two lines\n","spark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\n","spark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n","#Load from the default lakehouse, make sure you click on the pin <=============\n","from pyspark.sql.types import *\n","df = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/DimAccount.csv\")\n","df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/DimAccount\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":null,"statement_id":null,"state":"waiting","livy_statement_state":null,"queued_time":"2023-07-01T19:15:55.6681926Z","session_start_time":null,"execution_start_time":null,"execution_finish_time":null,"spark_jobs":null,"parent_msg_id":"ca067fa0-2067-4419-a201-ac81ceef0c52"},"text/plain":"StatementMeta(, , , Waiting, )"},"metadata":{}}],"execution_count":4,"metadata":{}},{"cell_type":"code","source":["#V order thing you can ignore those two lines\r\n","spark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\r\n","spark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\r\n","#Load from the default lakehouse, make sure you click on the pin <=============\r\n","from pyspark.sql.types import *\r\n","df = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/DimCurrency.csv\")\r\n","df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/DimCurrency\")"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}}},{"cell_type":"code","source":["#Thanks to MIM for help with this code\r\n","spark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\r\n","spark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\r\n","#Load from the default lakehouse, make sure you click on the pin <=============\r\n","from pyspark.sql.types import *\r\n","df = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/DimCustomer.csv\")\r\n","df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/DimCustomer\")"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}}},{"cell_type":"code","source":["#V order thing you can ignore those two lines\r\n","spark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\r\n","spark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\r\n","#Load from the default lakehouse, make sure you click on the pin <=============\r\n","from pyspark.sql.types import *\r\n","df = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/DimCustomer.csv\")\r\n","df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/DimCustomer\")"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}}},{"cell_type":"code","source":["#Thanks to MIM for help with this code\r\n","spark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\r\n","spark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\r\n","#Load from the default lakehouse, make sure you click on the pin <=============\r\n","from pyspark.sql.types import *\r\n","df = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/DimDate.csv\")\r\n","df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/DimDate\")"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}}},{"cell_type":"code","source":["#V order thing you can ignore those two lines\r\n","spark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\r\n","spark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\r\n","#Load from the default lakehouse, make sure you click on the pin <=============\r\n","from pyspark.sql.types import *\r\n","df = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/DimDepartmentGroup.csv\")\r\n","df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/DimDepartmentGroup\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":null,"statement_id":null,"state":"cancelled","livy_statement_state":null,"queued_time":"2023-07-01T19:15:42.7121898Z","session_start_time":null,"execution_start_time":null,"execution_finish_time":"2023-07-01T19:15:43.6020557Z","spark_jobs":null,"parent_msg_id":"2d8d83ec-612e-4853-b15c-a26553c03f89"},"text/plain":"StatementMeta(, , , Cancelled, )"},"metadata":{}}],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}}},{"cell_type":"code","source":["#V order thing you can ignore those two lines\r\n","spark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\r\n","spark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\r\n","#Load from the default lakehouse, make sure you click on the pin <=============\r\n","from pyspark.sql.types import *\r\n","df = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/DimEmployee.csv\")\r\n","df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/DimEmployee\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":null,"statement_id":null,"state":"cancelled","livy_statement_state":null,"queued_time":"2023-07-01T19:15:42.8248561Z","session_start_time":null,"execution_start_time":null,"execution_finish_time":"2023-07-01T19:15:43.6023091Z","spark_jobs":null,"parent_msg_id":"ff75d3f5-0667-4ed0-9879-de2b547045da"},"text/plain":"StatementMeta(, , , Cancelled, )"},"metadata":{}}],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}}},{"cell_type":"code","source":["#Thanks to MIM for help with this code\r\n","spark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\r\n","spark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\r\n","#Load from the default lakehouse, make sure you click on the pin <=============\r\n","from pyspark.sql.types import *\r\n","df = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/DimGeography.csv\")\r\n","df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/DimGeography\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":null,"statement_id":null,"state":"cancelled","livy_statement_state":null,"queued_time":"2023-07-01T19:15:42.9010096Z","session_start_time":null,"execution_start_time":null,"execution_finish_time":"2023-07-01T19:15:43.6025719Z","spark_jobs":null,"parent_msg_id":"c7a24469-af2d-4282-8cd2-11f5179dbcf1"},"text/plain":"StatementMeta(, , , Cancelled, )"},"metadata":{}}],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}}},{"cell_type":"code","source":["#V order thing you can ignore those two lines\r\n","spark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\r\n","spark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\r\n","#Load from the default lakehouse, make sure you click on the pin <=============\r\n","from pyspark.sql.types import *\r\n","df = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/DimOrganization.csv\")\r\n","df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/DimOrganization\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":null,"statement_id":null,"state":"cancelled","livy_statement_state":null,"queued_time":"2023-07-01T19:15:42.9960397Z","session_start_time":null,"execution_start_time":null,"execution_finish_time":"2023-07-01T19:15:43.6028297Z","spark_jobs":null,"parent_msg_id":"db64e8b9-5787-4ab2-8398-58af67b8e307"},"text/plain":"StatementMeta(, , , Cancelled, )"},"metadata":{}}],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}}},{"cell_type":"code","source":["#Thanks to MIM for help with this code\r\n","spark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\r\n","spark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\r\n","#Load from the default lakehouse, make sure you click on the pin <=============\r\n","from pyspark.sql.types import *\r\n","df = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/DimProduct.csv\")\r\n","df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/DimProduct\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":null,"statement_id":null,"state":"cancelled","livy_statement_state":null,"queued_time":"2023-07-01T19:15:43.0817456Z","session_start_time":null,"execution_start_time":null,"execution_finish_time":"2023-07-01T19:15:43.6030673Z","spark_jobs":null,"parent_msg_id":"d3dfe66f-b915-4eb7-aa84-9284080eba74"},"text/plain":"StatementMeta(, , , Cancelled, )"},"metadata":{}}],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}}},{"cell_type":"code","source":["#V order thing you can ignore those two lines\r\n","spark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\r\n","spark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\r\n","#Load from the default lakehouse, make sure you click on the pin <=============\r\n","from pyspark.sql.types import *\r\n","df = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/DimProductCategory.csv\")\r\n","df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/DimProductCategory\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":null,"statement_id":null,"state":"cancelled","livy_statement_state":null,"queued_time":"2023-07-01T19:15:43.152002Z","session_start_time":null,"execution_start_time":null,"execution_finish_time":"2023-07-01T19:15:43.6032754Z","spark_jobs":null,"parent_msg_id":"5a9f7278-a577-4a69-ab36-c204f3a3b410"},"text/plain":"StatementMeta(, , , Cancelled, )"},"metadata":{}}],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}}},{"cell_type":"code","source":["#Thanks to MIM for help with this code\r\n","spark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\r\n","spark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\r\n","#Load from the default lakehouse, make sure you click on the pin <=============\r\n","from pyspark.sql.types import *\r\n","df = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/DimProductSubcategory.csv\")\r\n","df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/DimProductSubcategory\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":null,"statement_id":null,"state":"cancelled","livy_statement_state":null,"queued_time":"2023-07-01T19:15:43.2240924Z","session_start_time":null,"execution_start_time":null,"execution_finish_time":"2023-07-01T19:15:43.6037288Z","spark_jobs":null,"parent_msg_id":"464d141c-6488-4857-8312-22249933b59d"},"text/plain":"StatementMeta(, , , Cancelled, )"},"metadata":{}}],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}}},{"cell_type":"code","source":["#V order thing you can ignore those two lines\r\n","spark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\r\n","spark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\r\n","#Load from the default lakehouse, make sure you click on the pin <=============\r\n","from pyspark.sql.types import *\r\n","df = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/DimPromotion.csv\")\r\n","df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/DimPromotion\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":null,"statement_id":null,"state":"cancelled","livy_statement_state":null,"queued_time":"2023-07-01T19:15:43.292168Z","session_start_time":null,"execution_start_time":null,"execution_finish_time":"2023-07-01T19:15:43.6039826Z","spark_jobs":null,"parent_msg_id":"7d908377-bec0-48c4-b718-ec842366c400"},"text/plain":"StatementMeta(, , , Cancelled, )"},"metadata":{}}],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}}},{"cell_type":"code","source":["#V order thing you can ignore those two lines\r\n","spark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\r\n","spark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\r\n","#Load from the default lakehouse, make sure you click on the pin <=============\r\n","from pyspark.sql.types import *\r\n","df = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/DimReseller.csv\")\r\n","df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/DimReseller\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":null,"statement_id":null,"state":"cancelled","livy_statement_state":null,"queued_time":"2023-07-01T19:15:43.3687068Z","session_start_time":null,"execution_start_time":null,"execution_finish_time":"2023-07-01T19:15:43.6041858Z","spark_jobs":null,"parent_msg_id":"f9ebafe3-3ee7-4962-8d62-0cf921c1b2fb"},"text/plain":"StatementMeta(, , , Cancelled, )"},"metadata":{}}],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}}},{"cell_type":"code","source":["#Thanks to MIM for help with this code\r\n","spark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\r\n","spark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\r\n","#Load from the default lakehouse, make sure you click on the pin <=============\r\n","from pyspark.sql.types import *\r\n","df = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/DimSalesReason.csv\")\r\n","df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/DimSalesReason\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":null,"statement_id":null,"state":"cancelled","livy_statement_state":null,"queued_time":"2023-07-01T19:15:43.4485183Z","session_start_time":null,"execution_start_time":null,"execution_finish_time":"2023-07-01T19:15:43.6043426Z","spark_jobs":null,"parent_msg_id":"899bf22f-deaf-446c-bb08-8571f171baf7"},"text/plain":"StatementMeta(, , , Cancelled, )"},"metadata":{}}],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}}},{"cell_type":"code","source":["#Thanks to MIM for help with this code\r\n","spark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\r\n","spark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\r\n","#Load from the default lakehouse, make sure you click on the pin <=============\r\n","from pyspark.sql.types import *\r\n","df = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/DimSalesTerritory.csv\")\r\n","df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/DimSalesTerritory\")"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}}},{"cell_type":"code","source":["#Thanks to MIM for help with this code\r\n","spark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\r\n","spark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\r\n","#Load from the default lakehouse, make sure you click on the pin <=============\r\n","from pyspark.sql.types import *\r\n","df = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/DimScenario.csv\")\r\n","df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/DimScenario\")"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}}},{"cell_type":"code","source":["#Thanks to MIM for help with this code\r\n","spark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\r\n","spark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\r\n","#Load from the default lakehouse, make sure you click on the pin <=============\r\n","from pyspark.sql.types import *\r\n","df = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/FactCallCenter.csv\")\r\n","df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/FactCallCenter\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"ce51e2d6-1144-4cc5-8c25-41ea190c8ca4","statement_id":25,"state":"finished","livy_statement_state":"available","queued_time":"2023-07-01T19:21:06.1609919Z","session_start_time":null,"execution_start_time":"2023-07-01T19:21:07.3696496Z","execution_finish_time":"2023-07-01T19:21:10.0041387Z","spark_jobs":{"numbers":{"UNKNOWN":0,"FAILED":0,"RUNNING":0,"SUCCEEDED":7},"jobs":[{"dataWritten":0,"dataRead":4487,"rowCount":50,"jobId":153,"name":"toString at String.java:2994","description":"Delta: Job group for statement 25:\n#Thanks to MIM for help with this code\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/FactCallCenter.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/FactCallCenter\"): Compute snapshot for version: 0","submissionTime":"2023-07-01T19:21:08.919GMT","completionTime":"2023-07-01T19:21:08.941GMT","stageIds":[219,220,221],"jobGroup":"25","status":"SUCCEEDED","numTasks":52,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":51,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":2,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":4487,"dataRead":1990,"rowCount":54,"jobId":152,"name":"toString at String.java:2994","description":"Delta: Job group for statement 25:\n#Thanks to MIM for help with this code\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/FactCallCenter.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/FactCallCenter\"): Compute snapshot for version: 0","submissionTime":"2023-07-01T19:21:08.617GMT","completionTime":"2023-07-01T19:21:08.907GMT","stageIds":[217,218],"jobGroup":"25","status":"SUCCEEDED","numTasks":51,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":1,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":1990,"dataRead":3114,"rowCount":8,"jobId":151,"name":"toString at String.java:2994","description":"Delta: Job group for statement 25:\n#Thanks to MIM for help with this code\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/FactCallCenter.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/FactCallCenter\"): Compute snapshot for version: 0","submissionTime":"2023-07-01T19:21:08.479GMT","completionTime":"2023-07-01T19:21:08.520GMT","stageIds":[216],"jobGroup":"25","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":0,"rowCount":0,"jobId":150,"name":"","description":"Job group for statement 25:\n#Thanks to MIM for help with this code\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/FactCallCenter.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/FactCallCenter\")","submissionTime":"2023-07-01T19:21:08.068GMT","completionTime":"2023-07-01T19:21:08.068GMT","stageIds":[],"jobGroup":"25","status":"SUCCEEDED","numTasks":0,"numActiveTasks":0,"numCompletedTasks":0,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":0,"numActiveStages":0,"numCompletedStages":0,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":10517,"dataRead":7095,"rowCount":240,"jobId":149,"name":"save at <unknown>:0","description":"Job group for statement 25:\n#Thanks to MIM for help with this code\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/FactCallCenter.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/FactCallCenter\")","submissionTime":"2023-07-01T19:21:07.750GMT","completionTime":"2023-07-01T19:21:07.989GMT","stageIds":[215,214],"jobGroup":"25","status":"SUCCEEDED","numTasks":2,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":1,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":7095,"dataRead":11148,"rowCount":240,"jobId":148,"name":"save at <unknown>:0","description":"Job group for statement 25:\n#Thanks to MIM for help with this code\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/FactCallCenter.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/FactCallCenter\")","submissionTime":"2023-07-01T19:21:07.675GMT","completionTime":"2023-07-01T19:21:07.716GMT","stageIds":[213],"jobGroup":"25","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":11148,"rowCount":1,"jobId":147,"name":"load at <unknown>:0","description":"Job group for statement 25:\n#Thanks to MIM for help with this code\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/FactCallCenter.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/FactCallCenter\")","submissionTime":"2023-07-01T19:21:07.440GMT","completionTime":"2023-07-01T19:21:07.478GMT","stageIds":[212],"jobGroup":"25","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}}],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"59f70cfc-05f8-4ee1-ade3-98e123a11724"},"text/plain":"StatementMeta(, ce51e2d6-1144-4cc5-8c25-41ea190c8ca4, 25, Finished, Available)"},"metadata":{}}],"execution_count":23,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}}},{"cell_type":"code","source":["#Thanks to MIM for help with this code\r\n","spark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\r\n","spark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\r\n","#Load from the default lakehouse, make sure you click on the pin <=============\r\n","from pyspark.sql.types import *\r\n","df = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/FactCurrencyRate.csv\")\r\n","df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/FactCurrencyRate\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"ce51e2d6-1144-4cc5-8c25-41ea190c8ca4","statement_id":26,"state":"finished","livy_statement_state":"available","queued_time":"2023-07-01T19:21:06.2521196Z","session_start_time":null,"execution_start_time":"2023-07-01T19:21:10.4493419Z","execution_finish_time":"2023-07-01T19:21:13.1598181Z","spark_jobs":{"numbers":{"UNKNOWN":0,"FAILED":0,"RUNNING":0,"SUCCEEDED":7},"jobs":[{"dataWritten":0,"dataRead":4351,"rowCount":50,"jobId":160,"name":"toString at String.java:2994","description":"Delta: Job group for statement 26:\n#Thanks to MIM for help with this code\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/FactCurrencyRate.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/FactCurrencyRate\"): Compute snapshot for version: 0","submissionTime":"2023-07-01T19:21:12.230GMT","completionTime":"2023-07-01T19:21:12.262GMT","stageIds":[230,231,229],"jobGroup":"26","status":"SUCCEEDED","numTasks":52,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":51,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":2,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":4351,"dataRead":1666,"rowCount":54,"jobId":159,"name":"toString at String.java:2994","description":"Delta: Job group for statement 26:\n#Thanks to MIM for help with this code\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/FactCurrencyRate.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/FactCurrencyRate\"): Compute snapshot for version: 0","submissionTime":"2023-07-01T19:21:11.860GMT","completionTime":"2023-07-01T19:21:12.216GMT","stageIds":[227,228],"jobGroup":"26","status":"SUCCEEDED","numTasks":51,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":1,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":1666,"dataRead":1776,"rowCount":8,"jobId":158,"name":"toString at String.java:2994","description":"Delta: Job group for statement 26:\n#Thanks to MIM for help with this code\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/FactCurrencyRate.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/FactCurrencyRate\"): Compute snapshot for version: 0","submissionTime":"2023-07-01T19:21:11.706GMT","completionTime":"2023-07-01T19:21:11.758GMT","stageIds":[226],"jobGroup":"26","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":0,"rowCount":0,"jobId":157,"name":"","description":"Job group for statement 26:\n#Thanks to MIM for help with this code\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/FactCurrencyRate.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/FactCurrencyRate\")","submissionTime":"2023-07-01T19:21:11.281GMT","completionTime":"2023-07-01T19:21:11.281GMT","stageIds":[],"jobGroup":"26","status":"SUCCEEDED","numTasks":0,"numActiveTasks":0,"numCompletedTasks":0,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":0,"numActiveStages":0,"numCompletedStages":0,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":280599,"dataRead":432256,"rowCount":28528,"jobId":156,"name":"save at <unknown>:0","description":"Job group for statement 26:\n#Thanks to MIM for help with this code\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/FactCurrencyRate.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/FactCurrencyRate\")","submissionTime":"2023-07-01T19:21:10.868GMT","completionTime":"2023-07-01T19:21:11.174GMT","stageIds":[224,225],"jobGroup":"26","status":"SUCCEEDED","numTasks":2,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":1,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":432256,"dataRead":1049978,"rowCount":28528,"jobId":155,"name":"save at <unknown>:0","description":"Job group for statement 26:\n#Thanks to MIM for help with this code\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/FactCurrencyRate.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/FactCurrencyRate\")","submissionTime":"2023-07-01T19:21:10.763GMT","completionTime":"2023-07-01T19:21:10.835GMT","stageIds":[223],"jobGroup":"26","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":65536,"rowCount":1,"jobId":154,"name":"load at <unknown>:0","description":"Job group for statement 26:\n#Thanks to MIM for help with this code\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/FactCurrencyRate.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/FactCurrencyRate\")","submissionTime":"2023-07-01T19:21:10.520GMT","completionTime":"2023-07-01T19:21:10.583GMT","stageIds":[222],"jobGroup":"26","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}}],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"648c31e3-67d4-4414-9339-aa00837f6468"},"text/plain":"StatementMeta(, ce51e2d6-1144-4cc5-8c25-41ea190c8ca4, 26, Finished, Available)"},"metadata":{}}],"execution_count":24,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}}},{"cell_type":"code","source":["#Thanks to MIM for help with this code\r\n","spark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\r\n","spark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\r\n","#Load from the default lakehouse, make sure you click on the pin\r\n","from pyspark.sql.types import *\r\n","df = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/FactFinance.csv\")\r\n","df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/FactFinance\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"ce51e2d6-1144-4cc5-8c25-41ea190c8ca4","statement_id":27,"state":"finished","livy_statement_state":"available","queued_time":"2023-07-01T19:21:06.3243346Z","session_start_time":null,"execution_start_time":"2023-07-01T19:21:13.7982015Z","execution_finish_time":"2023-07-01T19:21:16.4778539Z","spark_jobs":{"numbers":{"UNKNOWN":0,"FAILED":0,"RUNNING":0,"SUCCEEDED":7},"jobs":[{"dataWritten":0,"dataRead":4384,"rowCount":50,"jobId":167,"name":"toString at String.java:2994","description":"Delta: Job group for statement 27:\n#Thanks to MIM for help with this code\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/FactFinance.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/FactFinance\"): Compute snapshot for version: 0","submissionTime":"2023-07-01T19:21:15.685GMT","completionTime":"2023-07-01T19:21:15.706GMT","stageIds":[241,239,240],"jobGroup":"27","status":"SUCCEEDED","numTasks":52,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":51,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":2,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":4384,"dataRead":1720,"rowCount":54,"jobId":166,"name":"toString at String.java:2994","description":"Delta: Job group for statement 27:\n#Thanks to MIM for help with this code\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/FactFinance.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/FactFinance\"): Compute snapshot for version: 0","submissionTime":"2023-07-01T19:21:15.351GMT","completionTime":"2023-07-01T19:21:15.670GMT","stageIds":[237,238],"jobGroup":"27","status":"SUCCEEDED","numTasks":51,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":1,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":1720,"dataRead":2155,"rowCount":8,"jobId":165,"name":"toString at String.java:2994","description":"Delta: Job group for statement 27:\n#Thanks to MIM for help with this code\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/FactFinance.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/FactFinance\"): Compute snapshot for version: 0","submissionTime":"2023-07-01T19:21:15.187GMT","completionTime":"2023-07-01T19:21:15.235GMT","stageIds":[236],"jobGroup":"27","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":0,"rowCount":0,"jobId":164,"name":"","description":"Job group for statement 27:\n#Thanks to MIM for help with this code\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/FactFinance.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/FactFinance\")","submissionTime":"2023-07-01T19:21:14.782GMT","completionTime":"2023-07-01T19:21:14.782GMT","stageIds":[],"jobGroup":"27","status":"SUCCEEDED","numTasks":0,"numActiveTasks":0,"numCompletedTasks":0,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":0,"numActiveStages":0,"numCompletedStages":0,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":438299,"dataRead":880396,"rowCount":78818,"jobId":163,"name":"save at <unknown>:0","description":"Job group for statement 27:\n#Thanks to MIM for help with this code\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/FactFinance.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/FactFinance\")","submissionTime":"2023-07-01T19:21:14.302GMT","completionTime":"2023-07-01T19:21:14.689GMT","stageIds":[234,235],"jobGroup":"27","status":"SUCCEEDED","numTasks":2,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":1,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":880396,"dataRead":2305234,"rowCount":78818,"jobId":162,"name":"save at <unknown>:0","description":"Job group for statement 27:\n#Thanks to MIM for help with this code\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/FactFinance.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/FactFinance\")","submissionTime":"2023-07-01T19:21:14.090GMT","completionTime":"2023-07-01T19:21:14.267GMT","stageIds":[233],"jobGroup":"27","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":65536,"rowCount":1,"jobId":161,"name":"load at <unknown>:0","description":"Job group for statement 27:\n#Thanks to MIM for help with this code\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/FactFinance.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/FactFinance\")","submissionTime":"2023-07-01T19:21:13.847GMT","completionTime":"2023-07-01T19:21:13.925GMT","stageIds":[232],"jobGroup":"27","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}}],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"c3d8e172-2bdc-45e0-8576-2808e0e91f3f"},"text/plain":"StatementMeta(, ce51e2d6-1144-4cc5-8c25-41ea190c8ca4, 27, Finished, Available)"},"metadata":{}}],"execution_count":25,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}}},{"cell_type":"code","source":["#Thanks to MIM for help with this code\r\n","spark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\r\n","spark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\r\n","#Load from the default lakehouse, make sure you click on the pin <=============\r\n","from pyspark.sql.types import *\r\n","df = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/FactInternetSales.csv\")\r\n","df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/FactInternetSales\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"ce51e2d6-1144-4cc5-8c25-41ea190c8ca4","statement_id":28,"state":"finished","livy_statement_state":"available","queued_time":"2023-07-01T19:21:06.4411558Z","session_start_time":null,"execution_start_time":"2023-07-01T19:21:16.8422045Z","execution_finish_time":"2023-07-01T19:21:20.7778106Z","spark_jobs":{"numbers":{"UNKNOWN":0,"FAILED":0,"RUNNING":0,"SUCCEEDED":7},"jobs":[{"dataWritten":0,"dataRead":4609,"rowCount":50,"jobId":174,"name":"toString at String.java:2994","description":"Delta: Job group for statement 28:\n#Thanks to MIM for help with this code\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/FactInternetSales.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/FactInternetSales\"): Compute snapshot for version: 0","submissionTime":"2023-07-01T19:21:19.354GMT","completionTime":"2023-07-01T19:21:19.374GMT","stageIds":[251,249,250],"jobGroup":"28","status":"SUCCEEDED","numTasks":52,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":51,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":2,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":4609,"dataRead":2489,"rowCount":54,"jobId":173,"name":"toString at String.java:2994","description":"Delta: Job group for statement 28:\n#Thanks to MIM for help with this code\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/FactInternetSales.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/FactInternetSales\"): Compute snapshot for version: 0","submissionTime":"2023-07-01T19:21:19.037GMT","completionTime":"2023-07-01T19:21:19.341GMT","stageIds":[248,247],"jobGroup":"28","status":"SUCCEEDED","numTasks":51,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":1,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":2489,"dataRead":5174,"rowCount":8,"jobId":172,"name":"toString at String.java:2994","description":"Delta: Job group for statement 28:\n#Thanks to MIM for help with this code\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/FactInternetSales.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/FactInternetSales\"): Compute snapshot for version: 0","submissionTime":"2023-07-01T19:21:18.903GMT","completionTime":"2023-07-01T19:21:18.941GMT","stageIds":[246],"jobGroup":"28","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":0,"rowCount":0,"jobId":171,"name":"","description":"Job group for statement 28:\n#Thanks to MIM for help with this code\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/FactInternetSales.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/FactInternetSales\")","submissionTime":"2023-07-01T19:21:18.516GMT","completionTime":"2023-07-01T19:21:18.516GMT","stageIds":[],"jobGroup":"28","status":"SUCCEEDED","numTasks":0,"numActiveTasks":0,"numCompletedTasks":0,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":0,"numActiveStages":0,"numCompletedStages":0,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":1068305,"dataRead":2699801,"rowCount":120796,"jobId":170,"name":"save at <unknown>:0","description":"Job group for statement 28:\n#Thanks to MIM for help with this code\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/FactInternetSales.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/FactInternetSales\")","submissionTime":"2023-07-01T19:21:17.633GMT","completionTime":"2023-07-01T19:21:18.420GMT","stageIds":[245,244],"jobGroup":"28","status":"SUCCEEDED","numTasks":4,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":3,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":2699801,"dataRead":12401895,"rowCount":120796,"jobId":169,"name":"save at <unknown>:0","description":"Job group for statement 28:\n#Thanks to MIM for help with this code\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/FactInternetSales.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/FactInternetSales\")","submissionTime":"2023-07-01T19:21:17.247GMT","completionTime":"2023-07-01T19:21:17.597GMT","stageIds":[243],"jobGroup":"28","status":"SUCCEEDED","numTasks":3,"numActiveTasks":0,"numCompletedTasks":3,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":3,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":65536,"rowCount":1,"jobId":168,"name":"load at <unknown>:0","description":"Job group for statement 28:\n#Thanks to MIM for help with this code\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/FactInternetSales.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/FactInternetSales\")","submissionTime":"2023-07-01T19:21:16.887GMT","completionTime":"2023-07-01T19:21:17.026GMT","stageIds":[242],"jobGroup":"28","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}}],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"a1ed001d-bb1b-46b2-8f23-dd49709f8440"},"text/plain":"StatementMeta(, ce51e2d6-1144-4cc5-8c25-41ea190c8ca4, 28, Finished, Available)"},"metadata":{}}],"execution_count":26,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}}},{"cell_type":"code","source":["#Thanks to MIM for help with this code\r\n","spark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\r\n","spark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\r\n","#Load from the default lakehouse, make sure you click on the pin <=============\r\n","from pyspark.sql.types import *\r\n","df = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/FactProductInventory.csv\")\r\n","df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/FactProductInventory\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"ce51e2d6-1144-4cc5-8c25-41ea190c8ca4","statement_id":30,"state":"finished","livy_statement_state":"available","queued_time":"2023-07-01T19:21:34.226978Z","session_start_time":null,"execution_start_time":"2023-07-01T19:21:34.6199339Z","execution_finish_time":"2023-07-01T19:21:39.9344289Z","spark_jobs":{"numbers":{"UNKNOWN":0,"FAILED":0,"RUNNING":0,"SUCCEEDED":7},"jobs":[{"dataWritten":0,"dataRead":4381,"rowCount":50,"jobId":181,"name":"toString at String.java:2994","description":"Delta: Job group for statement 30:\n#Thanks to MIM for help with this code\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/FactProductInventory.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/FactProductInventory\"): Compute snapshot for version: 1","submissionTime":"2023-07-01T19:21:38.871GMT","completionTime":"2023-07-01T19:21:38.891GMT","stageIds":[263,261,262],"jobGroup":"30","status":"SUCCEEDED","numTasks":53,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":52,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":2,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":4381,"dataRead":3415,"rowCount":57,"jobId":180,"name":"toString at String.java:2994","description":"Delta: Job group for statement 30:\n#Thanks to MIM for help with this code\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/FactProductInventory.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/FactProductInventory\"): Compute snapshot for version: 1","submissionTime":"2023-07-01T19:21:38.541GMT","completionTime":"2023-07-01T19:21:38.859GMT","stageIds":[259,260],"jobGroup":"30","status":"SUCCEEDED","numTasks":52,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":2,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":3415,"dataRead":3341,"rowCount":14,"jobId":179,"name":"toString at String.java:2994","description":"Delta: Job group for statement 30:\n#Thanks to MIM for help with this code\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/FactProductInventory.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/FactProductInventory\"): Compute snapshot for version: 1","submissionTime":"2023-07-01T19:21:38.391GMT","completionTime":"2023-07-01T19:21:38.437GMT","stageIds":[258],"jobGroup":"30","status":"SUCCEEDED","numTasks":2,"numActiveTasks":0,"numCompletedTasks":2,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":2,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":1895,"rowCount":3,"jobId":178,"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","description":"Job group for statement 30:\n#Thanks to MIM for help with this code\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/FactProductInventory.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/FactProductInventory\")","submissionTime":"2023-07-01T19:21:37.932GMT","completionTime":"2023-07-01T19:21:38.017GMT","stageIds":[256,257],"jobGroup":"30","status":"SUCCEEDED","numTasks":51,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":1,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":2991229,"dataRead":10367345,"rowCount":1552572,"jobId":177,"name":"save at <unknown>:0","description":"Job group for statement 30:\n#Thanks to MIM for help with this code\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/FactProductInventory.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/FactProductInventory\")","submissionTime":"2023-07-01T19:21:36.359GMT","completionTime":"2023-07-01T19:21:37.842GMT","stageIds":[255,254],"jobGroup":"30","status":"SUCCEEDED","numTasks":9,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":8,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":10367345,"dataRead":31535642,"rowCount":1552572,"jobId":176,"name":"save at <unknown>:0","description":"Job group for statement 30:\n#Thanks to MIM for help with this code\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/FactProductInventory.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/FactProductInventory\")","submissionTime":"2023-07-01T19:21:35.614GMT","completionTime":"2023-07-01T19:21:36.320GMT","stageIds":[253],"jobGroup":"30","status":"SUCCEEDED","numTasks":8,"numActiveTasks":0,"numCompletedTasks":8,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":8,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":65536,"rowCount":1,"jobId":175,"name":"load at <unknown>:0","description":"Job group for statement 30:\n#Thanks to MIM for help with this code\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/FactProductInventory.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/FactProductInventory\")","submissionTime":"2023-07-01T19:21:34.698GMT","completionTime":"2023-07-01T19:21:34.843GMT","stageIds":[252],"jobGroup":"30","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}}],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"0d620b14-3d53-44d0-a7cd-b7e0c0bc924e"},"text/plain":"StatementMeta(, ce51e2d6-1144-4cc5-8c25-41ea190c8ca4, 30, Finished, Available)"},"metadata":{}}],"execution_count":28,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}}},{"cell_type":"code","source":["#Thanks to MIM for help with this code\r\n","spark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\r\n","spark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\r\n","#Load from the default lakehouse, make sure you click on the pin <=============\r\n","from pyspark.sql.types import *\r\n","df = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/FactResellerSales.csv\")\r\n","df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/FactResellerSales\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"ce51e2d6-1144-4cc5-8c25-41ea190c8ca4","statement_id":31,"state":"finished","livy_statement_state":"available","queued_time":"2023-07-01T19:21:34.3125182Z","session_start_time":null,"execution_start_time":"2023-07-01T19:21:40.4124628Z","execution_finish_time":"2023-07-01T19:21:43.0397957Z","spark_jobs":{"numbers":{"UNKNOWN":0,"FAILED":0,"RUNNING":0,"SUCCEEDED":7},"jobs":[{"dataWritten":0,"dataRead":4634,"rowCount":50,"jobId":188,"name":"toString at String.java:2994","description":"Delta: Job group for statement 31:\n#Thanks to MIM for help with this code\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/FactResellerSales.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/FactResellerSales\"): Compute snapshot for version: 1","submissionTime":"2023-07-01T19:21:42.757GMT","completionTime":"2023-07-01T19:21:42.777GMT","stageIds":[273,274,275],"jobGroup":"31","status":"SUCCEEDED","numTasks":53,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":52,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":2,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":4634,"dataRead":5151,"rowCount":57,"jobId":187,"name":"toString at String.java:2994","description":"Delta: Job group for statement 31:\n#Thanks to MIM for help with this code\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/FactResellerSales.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/FactResellerSales\"): Compute snapshot for version: 1","submissionTime":"2023-07-01T19:21:42.428GMT","completionTime":"2023-07-01T19:21:42.742GMT","stageIds":[271,272],"jobGroup":"31","status":"SUCCEEDED","numTasks":52,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":2,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":5151,"dataRead":8496,"rowCount":14,"jobId":186,"name":"toString at String.java:2994","description":"Delta: Job group for statement 31:\n#Thanks to MIM for help with this code\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/FactResellerSales.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/FactResellerSales\"): Compute snapshot for version: 1","submissionTime":"2023-07-01T19:21:42.247GMT","completionTime":"2023-07-01T19:21:42.286GMT","stageIds":[270],"jobGroup":"31","status":"SUCCEEDED","numTasks":2,"numActiveTasks":0,"numCompletedTasks":2,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":2,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":2904,"rowCount":3,"jobId":185,"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","description":"Job group for statement 31:\n#Thanks to MIM for help with this code\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/FactResellerSales.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/FactResellerSales\")","submissionTime":"2023-07-01T19:21:41.776GMT","completionTime":"2023-07-01T19:21:41.854GMT","stageIds":[269,268],"jobGroup":"31","status":"SUCCEEDED","numTasks":51,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":1,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":1217252,"dataRead":4224283,"rowCount":121710,"jobId":184,"name":"save at <unknown>:0","description":"Job group for statement 31:\n#Thanks to MIM for help with this code\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/FactResellerSales.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/FactResellerSales\")","submissionTime":"2023-07-01T19:21:40.985GMT","completionTime":"2023-07-01T19:21:41.709GMT","stageIds":[266,267],"jobGroup":"31","status":"SUCCEEDED","numTasks":5,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":4,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":4224283,"dataRead":14210885,"rowCount":121710,"jobId":183,"name":"save at <unknown>:0","description":"Job group for statement 31:\n#Thanks to MIM for help with this code\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/FactResellerSales.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/FactResellerSales\")","submissionTime":"2023-07-01T19:21:40.754GMT","completionTime":"2023-07-01T19:21:40.947GMT","stageIds":[265],"jobGroup":"31","status":"SUCCEEDED","numTasks":4,"numActiveTasks":0,"numCompletedTasks":4,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":4,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":65536,"rowCount":1,"jobId":182,"name":"load at <unknown>:0","description":"Job group for statement 31:\n#Thanks to MIM for help with this code\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/FactResellerSales.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/FactResellerSales\")","submissionTime":"2023-07-01T19:21:40.478GMT","completionTime":"2023-07-01T19:21:40.619GMT","stageIds":[264],"jobGroup":"31","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}}],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"566fc2e4-879b-4ceb-9583-63ec1d14f2d3"},"text/plain":"StatementMeta(, ce51e2d6-1144-4cc5-8c25-41ea190c8ca4, 31, Finished, Available)"},"metadata":{}}],"execution_count":29,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}}},{"cell_type":"code","source":["#Thanks to MIM for help with this code\r\n","spark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\r\n","spark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\r\n","#Load from the default lakehouse, make sure you click on the pin <=============\r\n","from pyspark.sql.types import *\r\n","df = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/FactSalesQuota.csv\")\r\n","df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/FactSalesQuota\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"ce51e2d6-1144-4cc5-8c25-41ea190c8ca4","statement_id":32,"state":"finished","livy_statement_state":"available","queued_time":"2023-07-01T19:21:34.3803177Z","session_start_time":null,"execution_start_time":"2023-07-01T19:21:43.4117541Z","execution_finish_time":"2023-07-01T19:21:46.1701619Z","spark_jobs":{"numbers":{"UNKNOWN":0,"FAILED":0,"RUNNING":0,"SUCCEEDED":7},"jobs":[{"dataWritten":0,"dataRead":4389,"rowCount":50,"jobId":195,"name":"toString at String.java:2994","description":"Delta: Job group for statement 32:\n#Thanks to MIM for help with this code\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/FactSalesQuota.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/FactSalesQuota\"): Compute snapshot for version: 1","submissionTime":"2023-07-01T19:21:45.044GMT","completionTime":"2023-07-01T19:21:45.073GMT","stageIds":[287,285,286],"jobGroup":"32","status":"SUCCEEDED","numTasks":53,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":52,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":2,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":4389,"dataRead":3391,"rowCount":57,"jobId":194,"name":"toString at String.java:2994","description":"Delta: Job group for statement 32:\n#Thanks to MIM for help with this code\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/FactSalesQuota.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/FactSalesQuota\"): Compute snapshot for version: 1","submissionTime":"2023-07-01T19:21:44.701GMT","completionTime":"2023-07-01T19:21:45.026GMT","stageIds":[284,283],"jobGroup":"32","status":"SUCCEEDED","numTasks":52,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":2,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":3391,"dataRead":3490,"rowCount":14,"jobId":193,"name":"toString at String.java:2994","description":"Delta: Job group for statement 32:\n#Thanks to MIM for help with this code\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/FactSalesQuota.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/FactSalesQuota\"): Compute snapshot for version: 1","submissionTime":"2023-07-01T19:21:44.553GMT","completionTime":"2023-07-01T19:21:44.592GMT","stageIds":[282],"jobGroup":"32","status":"SUCCEEDED","numTasks":2,"numActiveTasks":0,"numCompletedTasks":2,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":2,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":1908,"rowCount":3,"jobId":192,"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","description":"Job group for statement 32:\n#Thanks to MIM for help with this code\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/FactSalesQuota.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/FactSalesQuota\")","submissionTime":"2023-07-01T19:21:44.062GMT","completionTime":"2023-07-01T19:21:44.155GMT","stageIds":[281,280],"jobGroup":"32","status":"SUCCEEDED","numTasks":51,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":1,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":5959,"dataRead":3735,"rowCount":326,"jobId":191,"name":"save at <unknown>:0","description":"Job group for statement 32:\n#Thanks to MIM for help with this code\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/FactSalesQuota.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/FactSalesQuota\")","submissionTime":"2023-07-01T19:21:43.747GMT","completionTime":"2023-07-01T19:21:43.982GMT","stageIds":[278,279],"jobGroup":"32","status":"SUCCEEDED","numTasks":2,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":1,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":3735,"dataRead":9930,"rowCount":326,"jobId":190,"name":"save at <unknown>:0","description":"Job group for statement 32:\n#Thanks to MIM for help with this code\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/FactSalesQuota.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/FactSalesQuota\")","submissionTime":"2023-07-01T19:21:43.601GMT","completionTime":"2023-07-01T19:21:43.713GMT","stageIds":[277],"jobGroup":"32","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":9930,"rowCount":1,"jobId":189,"name":"load at <unknown>:0","description":"Job group for statement 32:\n#Thanks to MIM for help with this code\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/FactSalesQuota.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/FactSalesQuota\")","submissionTime":"2023-07-01T19:21:43.464GMT","completionTime":"2023-07-01T19:21:43.495GMT","stageIds":[276],"jobGroup":"32","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}}],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"79d6d78a-77a2-4094-9576-ff4c1b6075a2"},"text/plain":"StatementMeta(, ce51e2d6-1144-4cc5-8c25-41ea190c8ca4, 32, Finished, Available)"},"metadata":{}}],"execution_count":30,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}}},{"cell_type":"code","source":["#Thanks to MIM for help with this code\r\n","spark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\r\n","spark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\r\n","#Load from the default lakehouse, make sure you click on the pin <=============\r\n","from pyspark.sql.types import *\r\n","df = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/FactSurveyResponse.csv\")\r\n","df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/FactSurveyResponse\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"ce51e2d6-1144-4cc5-8c25-41ea190c8ca4","statement_id":33,"state":"finished","livy_statement_state":"available","queued_time":"2023-07-01T19:21:34.5282171Z","session_start_time":null,"execution_start_time":"2023-07-01T19:21:46.5296207Z","execution_finish_time":"2023-07-01T19:21:49.2292939Z","spark_jobs":{"numbers":{"UNKNOWN":0,"FAILED":0,"RUNNING":0,"SUCCEEDED":7},"jobs":[{"dataWritten":0,"dataRead":4397,"rowCount":50,"jobId":202,"name":"toString at String.java:2994","description":"Delta: Job group for statement 33:\n#Thanks to MIM for help with this code\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/FactSurveyResponse.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/FactSurveyResponse\"): Compute snapshot for version: 0","submissionTime":"2023-07-01T19:21:48.273GMT","completionTime":"2023-07-01T19:21:48.296GMT","stageIds":[296,297,295],"jobGroup":"33","status":"SUCCEEDED","numTasks":52,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":51,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":2,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":4397,"dataRead":1791,"rowCount":54,"jobId":201,"name":"toString at String.java:2994","description":"Delta: Job group for statement 33:\n#Thanks to MIM for help with this code\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/FactSurveyResponse.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/FactSurveyResponse\"): Compute snapshot for version: 0","submissionTime":"2023-07-01T19:21:47.873GMT","completionTime":"2023-07-01T19:21:48.256GMT","stageIds":[293,294],"jobGroup":"33","status":"SUCCEEDED","numTasks":51,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":1,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":1791,"dataRead":2384,"rowCount":8,"jobId":200,"name":"toString at String.java:2994","description":"Delta: Job group for statement 33:\n#Thanks to MIM for help with this code\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/FactSurveyResponse.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/FactSurveyResponse\"): Compute snapshot for version: 0","submissionTime":"2023-07-01T19:21:47.720GMT","completionTime":"2023-07-01T19:21:47.759GMT","stageIds":[292],"jobGroup":"33","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":0,"rowCount":0,"jobId":199,"name":"","description":"Job group for statement 33:\n#Thanks to MIM for help with this code\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/FactSurveyResponse.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/FactSurveyResponse\")","submissionTime":"2023-07-01T19:21:47.253GMT","completionTime":"2023-07-01T19:21:47.253GMT","stageIds":[],"jobGroup":"33","status":"SUCCEEDED","numTasks":0,"numActiveTasks":0,"numCompletedTasks":0,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":0,"numActiveStages":0,"numCompletedStages":0,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":36700,"dataRead":79111,"rowCount":5454,"jobId":198,"name":"save at <unknown>:0","description":"Job group for statement 33:\n#Thanks to MIM for help with this code\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/FactSurveyResponse.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/FactSurveyResponse\")","submissionTime":"2023-07-01T19:21:46.909GMT","completionTime":"2023-07-01T19:21:47.152GMT","stageIds":[291,290],"jobGroup":"33","status":"SUCCEEDED","numTasks":2,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":1,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":79111,"dataRead":188571,"rowCount":5454,"jobId":197,"name":"save at <unknown>:0","description":"Job group for statement 33:\n#Thanks to MIM for help with this code\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/FactSurveyResponse.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/FactSurveyResponse\")","submissionTime":"2023-07-01T19:21:46.833GMT","completionTime":"2023-07-01T19:21:46.874GMT","stageIds":[289],"jobGroup":"33","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":65536,"rowCount":1,"jobId":196,"name":"load at <unknown>:0","description":"Job group for statement 33:\n#Thanks to MIM for help with this code\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/FactSurveyResponse.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/FactSurveyResponse\")","submissionTime":"2023-07-01T19:21:46.578GMT","completionTime":"2023-07-01T19:21:46.620GMT","stageIds":[288],"jobGroup":"33","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}}],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"60474be0-5ede-4ef5-8136-871b6cdc7303"},"text/plain":"StatementMeta(, ce51e2d6-1144-4cc5-8c25-41ea190c8ca4, 33, Finished, Available)"},"metadata":{}}],"execution_count":31,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}}}],"metadata":{"language_info":{"name":"python"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"widgets":{},"kernel_info":{"name":"synapse_pyspark"},"save_output":true,"spark_compute":{"compute_id":"/trident/default","session_options":{"enableDebugMode":false,"conf":{}}},"notebook_environment":{},"synapse_widget":{"version":"0.1","state":{}},"trident":{"lakehouse":{"default_lakehouse":"cc090d57-494e-4f4c-a1ee-15604f15f062","known_lakehouses":[{"id":"62e855f8-1ce8-4585-9b33-606d5656bd18"},{"id":"cc090d57-494e-4f4c-a1ee-15604f15f062"}],"default_lakehouse_name":"AdventureWorksLH","default_lakehouse_workspace_id":"d512a9f4-0c75-4504-80fa-eaa50871f1f4"}}},"nbformat":4,"nbformat_minor":0}