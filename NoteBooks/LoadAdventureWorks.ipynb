{"cells":[{"cell_type":"code","source":["#V order thing you can ignore those two lines\n","spark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\n","spark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n","#Load from the default lakehouse, make sure you click on the pin <=============\n","from pyspark.sql.types import *\n","df = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/DimAccount.csv\")\n","df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/DimAccount\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"3c166b9a-f167-4f97-8747-8e1fdca2eefa","statement_id":5,"state":"finished","livy_statement_state":"available","queued_time":"2023-07-01T17:47:27.1685785Z","session_start_time":null,"execution_start_time":"2023-07-01T17:47:27.4704955Z","execution_finish_time":"2023-07-01T17:47:30.1631904Z","spark_jobs":{"numbers":{"RUNNING":0,"FAILED":0,"SUCCEEDED":7,"UNKNOWN":0},"jobs":[{"dataWritten":0,"dataRead":4429,"rowCount":50,"jobId":27,"name":"toString at String.java:2994","description":"Delta: Job group for statement 5:\n#V order thing you can ignore those two lines\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/DimAccount.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/DimAccount\"): Compute snapshot for version: 2","submissionTime":"2023-07-01T17:47:29.606GMT","completionTime":"2023-07-01T17:47:29.644GMT","stageIds":[42,43,41],"jobGroup":"5","status":"SUCCEEDED","numTasks":54,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":53,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":2,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":4429,"dataRead":5508,"rowCount":60,"jobId":26,"name":"toString at String.java:2994","description":"Delta: Job group for statement 5:\n#V order thing you can ignore those two lines\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/DimAccount.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/DimAccount\"): Compute snapshot for version: 2","submissionTime":"2023-07-01T17:47:29.042GMT","completionTime":"2023-07-01T17:47:29.587GMT","stageIds":[39,40],"jobGroup":"5","status":"SUCCEEDED","numTasks":53,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":3,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":5508,"dataRead":6252,"rowCount":20,"jobId":25,"name":"toString at String.java:2994","description":"Delta: Job group for statement 5:\n#V order thing you can ignore those two lines\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/DimAccount.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/DimAccount\"): Compute snapshot for version: 2","submissionTime":"2023-07-01T17:47:28.777GMT","completionTime":"2023-07-01T17:47:28.839GMT","stageIds":[38],"jobGroup":"5","status":"SUCCEEDED","numTasks":3,"numActiveTasks":0,"numCompletedTasks":3,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":3,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":2271,"rowCount":4,"jobId":24,"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","description":"Job group for statement 5:\n#V order thing you can ignore those two lines\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/DimAccount.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/DimAccount\")","submissionTime":"2023-07-01T17:47:28.243GMT","completionTime":"2023-07-01T17:47:28.380GMT","stageIds":[37,36],"jobGroup":"5","status":"SUCCEEDED","numTasks":52,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":2,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":8361,"dataRead":5395,"rowCount":198,"jobId":23,"name":"save at NativeMethodAccessorImpl.java:0","description":"Job group for statement 5:\n#V order thing you can ignore those two lines\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/DimAccount.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/DimAccount\")","submissionTime":"2023-07-01T17:47:27.904GMT","completionTime":"2023-07-01T17:47:28.130GMT","stageIds":[34,35],"jobGroup":"5","status":"SUCCEEDED","numTasks":2,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":1,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":5395,"dataRead":6622,"rowCount":198,"jobId":22,"name":"save at NativeMethodAccessorImpl.java:0","description":"Job group for statement 5:\n#V order thing you can ignore those two lines\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/DimAccount.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/DimAccount\")","submissionTime":"2023-07-01T17:47:27.776GMT","completionTime":"2023-07-01T17:47:27.824GMT","stageIds":[33],"jobGroup":"5","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":6622,"rowCount":1,"jobId":21,"name":"load at NativeMethodAccessorImpl.java:0","description":"Job group for statement 5:\n#V order thing you can ignore those two lines\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/DimAccount.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/DimAccount\")","submissionTime":"2023-07-01T17:47:27.568GMT","completionTime":"2023-07-01T17:47:27.618GMT","stageIds":[32],"jobGroup":"5","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}}],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"08d86b82-4277-4ba4-85a0-abba98e450b0"},"text/plain":"StatementMeta(, 3c166b9a-f167-4f97-8747-8e1fdca2eefa, 5, Finished, Available)"},"metadata":{}}],"execution_count":3,"metadata":{"advisor":{"adviceMetadata":"{\"artifactId\":\"4b6dc142-53a6-4302-92d0-b0982019ff67\",\"activityId\":\"3c166b9a-f167-4f97-8747-8e1fdca2eefa\",\"applicationId\":\"application_1688218711538_0001\",\"jobGroupId\":\"5\",\"advices\":{\"info\":1}}"}}},{"cell_type":"code","source":["#V order thing you can ignore those two lines\r\n","spark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\r\n","spark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\r\n","#Load from the default lakehouse, make sure you click on the pin <=============\r\n","from pyspark.sql.types import *\r\n","df = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/DimCurrency.csv\")\r\n","df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/DimCurrency\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"3c166b9a-f167-4f97-8747-8e1fdca2eefa","statement_id":6,"state":"finished","livy_statement_state":"available","queued_time":"2023-07-01T17:47:27.2847949Z","session_start_time":null,"execution_start_time":"2023-07-01T17:47:30.4357007Z","execution_finish_time":"2023-07-01T17:47:33.1609242Z","spark_jobs":{"numbers":{"RUNNING":0,"FAILED":0,"SUCCEEDED":7,"UNKNOWN":0},"jobs":[{"dataWritten":0,"dataRead":4324,"rowCount":50,"jobId":34,"name":"toString at String.java:2994","description":"Delta: Job group for statement 6:\n#V order thing you can ignore those two lines\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/DimCurrency.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/DimCurrency\"): Compute snapshot for version: 0","submissionTime":"2023-07-01T17:47:32.462GMT","completionTime":"2023-07-01T17:47:32.492GMT","stageIds":[51,52,53],"jobGroup":"6","status":"SUCCEEDED","numTasks":52,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":51,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":2,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":4324,"dataRead":1500,"rowCount":54,"jobId":33,"name":"toString at String.java:2994","description":"Delta: Job group for statement 6:\n#V order thing you can ignore those two lines\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/DimCurrency.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/DimCurrency\"): Compute snapshot for version: 0","submissionTime":"2023-07-01T17:47:31.818GMT","completionTime":"2023-07-01T17:47:32.395GMT","stageIds":[49,50],"jobGroup":"6","status":"SUCCEEDED","numTasks":51,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":1,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":1500,"dataRead":1440,"rowCount":8,"jobId":32,"name":"toString at String.java:2994","description":"Delta: Job group for statement 6:\n#V order thing you can ignore those two lines\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/DimCurrency.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/DimCurrency\"): Compute snapshot for version: 0","submissionTime":"2023-07-01T17:47:31.647GMT","completionTime":"2023-07-01T17:47:31.696GMT","stageIds":[48],"jobGroup":"6","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":0,"rowCount":0,"jobId":31,"name":"","description":"Job group for statement 6:\n#V order thing you can ignore those two lines\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/DimCurrency.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/DimCurrency\")","submissionTime":"2023-07-01T17:47:31.218GMT","completionTime":"2023-07-01T17:47:31.218GMT","stageIds":[],"jobGroup":"6","status":"SUCCEEDED","numTasks":0,"numActiveTasks":0,"numCompletedTasks":0,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":0,"numActiveStages":0,"numCompletedStages":0,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":4243,"dataRead":2947,"rowCount":210,"jobId":30,"name":"save at NativeMethodAccessorImpl.java:0","description":"Job group for statement 6:\n#V order thing you can ignore those two lines\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/DimCurrency.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/DimCurrency\")","submissionTime":"2023-07-01T17:47:30.832GMT","completionTime":"2023-07-01T17:47:31.063GMT","stageIds":[46,47],"jobGroup":"6","status":"SUCCEEDED","numTasks":2,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":1,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":2947,"dataRead":2256,"rowCount":210,"jobId":29,"name":"save at NativeMethodAccessorImpl.java:0","description":"Job group for statement 6:\n#V order thing you can ignore those two lines\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/DimCurrency.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/DimCurrency\")","submissionTime":"2023-07-01T17:47:30.734GMT","completionTime":"2023-07-01T17:47:30.798GMT","stageIds":[45],"jobGroup":"6","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":2256,"rowCount":1,"jobId":28,"name":"load at NativeMethodAccessorImpl.java:0","description":"Job group for statement 6:\n#V order thing you can ignore those two lines\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/DimCurrency.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/DimCurrency\")","submissionTime":"2023-07-01T17:47:30.512GMT","completionTime":"2023-07-01T17:47:30.551GMT","stageIds":[44],"jobGroup":"6","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}}],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"6cdc6ac7-6d2a-4760-828d-4796d002ac00"},"text/plain":"StatementMeta(, 3c166b9a-f167-4f97-8747-8e1fdca2eefa, 6, Finished, Available)"},"metadata":{}}],"execution_count":4,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}}},{"cell_type":"code","source":["#Thanks to MIM for help with this code\r\n","spark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\r\n","spark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\r\n","#Load from the default lakehouse, make sure you click on the pin <=============\r\n","from pyspark.sql.types import *\r\n","df = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/DimCustomer.csv\")\r\n","df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/DimCustomer\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"3c166b9a-f167-4f97-8747-8e1fdca2eefa","statement_id":7,"state":"finished","livy_statement_state":"available","queued_time":"2023-07-01T17:47:27.3879643Z","session_start_time":null,"execution_start_time":"2023-07-01T17:47:33.5240494Z","execution_finish_time":"2023-07-01T17:47:37.2586359Z","spark_jobs":{"numbers":{"RUNNING":0,"FAILED":0,"SUCCEEDED":7,"UNKNOWN":0},"jobs":[{"dataWritten":0,"dataRead":4653,"rowCount":50,"jobId":41,"name":"toString at String.java:2994","description":"Delta: Job group for statement 7:\n#Thanks to MIM for help with this code\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/DimCustomer.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/DimCustomer\"): Compute snapshot for version: 0","submissionTime":"2023-07-01T17:47:36.243GMT","completionTime":"2023-07-01T17:47:36.271GMT","stageIds":[63,61,62],"jobGroup":"7","status":"SUCCEEDED","numTasks":52,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":51,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":2,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":4653,"dataRead":2791,"rowCount":54,"jobId":40,"name":"toString at String.java:2994","description":"Delta: Job group for statement 7:\n#Thanks to MIM for help with this code\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/DimCustomer.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/DimCustomer\"): Compute snapshot for version: 0","submissionTime":"2023-07-01T17:47:35.721GMT","completionTime":"2023-07-01T17:47:36.216GMT","stageIds":[60,59],"jobGroup":"7","status":"SUCCEEDED","numTasks":51,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":1,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":2791,"dataRead":5667,"rowCount":8,"jobId":39,"name":"toString at String.java:2994","description":"Delta: Job group for statement 7:\n#Thanks to MIM for help with this code\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/DimCustomer.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/DimCustomer\"): Compute snapshot for version: 0","submissionTime":"2023-07-01T17:47:35.540GMT","completionTime":"2023-07-01T17:47:35.586GMT","stageIds":[58],"jobGroup":"7","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":0,"rowCount":0,"jobId":38,"name":"","description":"Job group for statement 7:\n#Thanks to MIM for help with this code\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/DimCustomer.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/DimCustomer\")","submissionTime":"2023-07-01T17:47:35.099GMT","completionTime":"2023-07-01T17:47:35.099GMT","stageIds":[],"jobGroup":"7","status":"SUCCEEDED","numTasks":0,"numActiveTasks":0,"numCompletedTasks":0,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":0,"numActiveStages":0,"numCompletedStages":0,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":943731,"dataRead":2525474,"rowCount":36968,"jobId":37,"name":"save at NativeMethodAccessorImpl.java:0","description":"Job group for statement 7:\n#Thanks to MIM for help with this code\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/DimCustomer.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/DimCustomer\")","submissionTime":"2023-07-01T17:47:34.406GMT","completionTime":"2023-07-01T17:47:34.993GMT","stageIds":[56,57],"jobGroup":"7","status":"SUCCEEDED","numTasks":3,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":2,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":2525474,"dataRead":4781007,"rowCount":36968,"jobId":36,"name":"save at NativeMethodAccessorImpl.java:0","description":"Job group for statement 7:\n#Thanks to MIM for help with this code\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/DimCustomer.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/DimCustomer\")","submissionTime":"2023-07-01T17:47:33.987GMT","completionTime":"2023-07-01T17:47:34.358GMT","stageIds":[55],"jobGroup":"7","status":"SUCCEEDED","numTasks":2,"numActiveTasks":0,"numCompletedTasks":2,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":2,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":65536,"rowCount":1,"jobId":35,"name":"load at NativeMethodAccessorImpl.java:0","description":"Job group for statement 7:\n#Thanks to MIM for help with this code\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/DimCustomer.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/DimCustomer\")","submissionTime":"2023-07-01T17:47:33.607GMT","completionTime":"2023-07-01T17:47:33.742GMT","stageIds":[54],"jobGroup":"7","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}}],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"4425e76f-9aa4-4ec3-9912-b12d426bc4d5"},"text/plain":"StatementMeta(, 3c166b9a-f167-4f97-8747-8e1fdca2eefa, 7, Finished, Available)"},"metadata":{}}],"execution_count":5,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"advisor":{"adviceMetadata":"{\"artifactId\":\"4b6dc142-53a6-4302-92d0-b0982019ff67\",\"activityId\":\"3c166b9a-f167-4f97-8747-8e1fdca2eefa\",\"applicationId\":\"application_1688218711538_0001\",\"jobGroupId\":\"7\",\"advices\":{\"info\":1}}"}}},{"cell_type":"code","source":["#V order thing you can ignore those two lines\r\n","spark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\r\n","spark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\r\n","#Load from the default lakehouse, make sure you click on the pin <=============\r\n","from pyspark.sql.types import *\r\n","df = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/DimCustomer.csv\")\r\n","df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/DimCustomer\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"3c166b9a-f167-4f97-8747-8e1fdca2eefa","statement_id":8,"state":"finished","livy_statement_state":"available","queued_time":"2023-07-01T17:47:27.5518621Z","session_start_time":null,"execution_start_time":"2023-07-01T17:47:37.5965223Z","execution_finish_time":"2023-07-01T17:47:40.2252015Z","spark_jobs":{"numbers":{"RUNNING":0,"FAILED":0,"SUCCEEDED":7,"UNKNOWN":0},"jobs":[{"dataWritten":0,"dataRead":4658,"rowCount":50,"jobId":48,"name":"toString at String.java:2994","description":"Delta: Job group for statement 8:\n#V order thing you can ignore those two lines\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/DimCustomer.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/DimCustomer\"): Compute snapshot for version: 1","submissionTime":"2023-07-01T17:47:39.960GMT","completionTime":"2023-07-01T17:47:40.001GMT","stageIds":[74,75,73],"jobGroup":"8","status":"SUCCEEDED","numTasks":53,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":52,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":2,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":4658,"dataRead":5348,"rowCount":57,"jobId":47,"name":"toString at String.java:2994","description":"Delta: Job group for statement 8:\n#V order thing you can ignore those two lines\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/DimCustomer.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/DimCustomer\"): Compute snapshot for version: 1","submissionTime":"2023-07-01T17:47:39.459GMT","completionTime":"2023-07-01T17:47:39.943GMT","stageIds":[71,72],"jobGroup":"8","status":"SUCCEEDED","numTasks":52,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":2,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":5348,"dataRead":8906,"rowCount":14,"jobId":46,"name":"toString at String.java:2994","description":"Delta: Job group for statement 8:\n#V order thing you can ignore those two lines\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/DimCustomer.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/DimCustomer\"): Compute snapshot for version: 1","submissionTime":"2023-07-01T17:47:39.219GMT","completionTime":"2023-07-01T17:47:39.268GMT","stageIds":[70],"jobGroup":"8","status":"SUCCEEDED","numTasks":2,"numActiveTasks":0,"numCompletedTasks":2,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":2,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":3017,"rowCount":3,"jobId":45,"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","description":"Job group for statement 8:\n#V order thing you can ignore those two lines\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/DimCustomer.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/DimCustomer\")","submissionTime":"2023-07-01T17:47:38.727GMT","completionTime":"2023-07-01T17:47:38.868GMT","stageIds":[68,69],"jobGroup":"8","status":"SUCCEEDED","numTasks":51,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":1,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":943731,"dataRead":2525474,"rowCount":36968,"jobId":44,"name":"save at NativeMethodAccessorImpl.java:0","description":"Job group for statement 8:\n#V order thing you can ignore those two lines\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/DimCustomer.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/DimCustomer\")","submissionTime":"2023-07-01T17:47:38.156GMT","completionTime":"2023-07-01T17:47:38.633GMT","stageIds":[66,67],"jobGroup":"8","status":"SUCCEEDED","numTasks":3,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":2,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":2525474,"dataRead":4781007,"rowCount":36968,"jobId":43,"name":"save at NativeMethodAccessorImpl.java:0","description":"Job group for statement 8:\n#V order thing you can ignore those two lines\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/DimCustomer.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/DimCustomer\")","submissionTime":"2023-07-01T17:47:37.930GMT","completionTime":"2023-07-01T17:47:38.113GMT","stageIds":[65],"jobGroup":"8","status":"SUCCEEDED","numTasks":2,"numActiveTasks":0,"numCompletedTasks":2,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":2,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":65536,"rowCount":1,"jobId":42,"name":"load at NativeMethodAccessorImpl.java:0","description":"Job group for statement 8:\n#V order thing you can ignore those two lines\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/DimCustomer.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/DimCustomer\")","submissionTime":"2023-07-01T17:47:37.670GMT","completionTime":"2023-07-01T17:47:37.771GMT","stageIds":[64],"jobGroup":"8","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}}],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"772b971b-c765-44a1-88a2-53a568807617"},"text/plain":"StatementMeta(, 3c166b9a-f167-4f97-8747-8e1fdca2eefa, 8, Finished, Available)"},"metadata":{}}],"execution_count":6,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"advisor":{"adviceMetadata":"{\"artifactId\":\"4b6dc142-53a6-4302-92d0-b0982019ff67\",\"activityId\":\"3c166b9a-f167-4f97-8747-8e1fdca2eefa\",\"applicationId\":\"application_1688218711538_0001\",\"jobGroupId\":\"8\",\"advices\":{\"info\":1}}"}}},{"cell_type":"code","source":["#Thanks to MIM for help with this code\r\n","spark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\r\n","spark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\r\n","#Load from the default lakehouse, make sure you click on the pin <=============\r\n","from pyspark.sql.types import *\r\n","df = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/DimDate.csv\")\r\n","df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/DimDate\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"3c166b9a-f167-4f97-8747-8e1fdca2eefa","statement_id":9,"state":"finished","livy_statement_state":"available","queued_time":"2023-07-01T17:47:27.7116796Z","session_start_time":null,"execution_start_time":"2023-07-01T17:47:40.568711Z","execution_finish_time":"2023-07-01T17:47:43.2284106Z","spark_jobs":{"numbers":{"RUNNING":0,"FAILED":0,"SUCCEEDED":7,"UNKNOWN":0},"jobs":[{"dataWritten":0,"dataRead":4507,"rowCount":50,"jobId":55,"name":"toString at String.java:2994","description":"Delta: Job group for statement 9:\n#Thanks to MIM for help with this code\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/DimDate.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/DimDate\"): Compute snapshot for version: 0","submissionTime":"2023-07-01T17:47:42.597GMT","completionTime":"2023-07-01T17:47:42.622GMT","stageIds":[84,85,83],"jobGroup":"9","status":"SUCCEEDED","numTasks":52,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":51,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":2,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":4507,"dataRead":2151,"rowCount":54,"jobId":54,"name":"toString at String.java:2994","description":"Delta: Job group for statement 9:\n#Thanks to MIM for help with this code\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/DimDate.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/DimDate\"): Compute snapshot for version: 0","submissionTime":"2023-07-01T17:47:42.075GMT","completionTime":"2023-07-01T17:47:42.583GMT","stageIds":[81,82],"jobGroup":"9","status":"SUCCEEDED","numTasks":51,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":1,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":2151,"dataRead":4131,"rowCount":8,"jobId":53,"name":"toString at String.java:2994","description":"Delta: Job group for statement 9:\n#Thanks to MIM for help with this code\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/DimDate.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/DimDate\"): Compute snapshot for version: 0","submissionTime":"2023-07-01T17:47:41.916GMT","completionTime":"2023-07-01T17:47:41.962GMT","stageIds":[80],"jobGroup":"9","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":0,"rowCount":0,"jobId":52,"name":"","description":"Job group for statement 9:\n#Thanks to MIM for help with this code\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/DimDate.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/DimDate\")","submissionTime":"2023-07-01T17:47:41.507GMT","completionTime":"2023-07-01T17:47:41.507GMT","stageIds":[],"jobGroup":"9","status":"SUCCEEDED","numTasks":0,"numActiveTasks":0,"numCompletedTasks":0,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":0,"numActiveStages":0,"numCompletedStages":0,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":59005,"dataRead":127313,"rowCount":7304,"jobId":51,"name":"save at NativeMethodAccessorImpl.java:0","description":"Job group for statement 9:\n#Thanks to MIM for help with this code\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/DimDate.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/DimDate\")","submissionTime":"2023-07-01T17:47:41.034GMT","completionTime":"2023-07-01T17:47:41.386GMT","stageIds":[78,79],"jobGroup":"9","status":"SUCCEEDED","numTasks":2,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":1,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":127313,"dataRead":355515,"rowCount":7304,"jobId":50,"name":"save at NativeMethodAccessorImpl.java:0","description":"Job group for statement 9:\n#Thanks to MIM for help with this code\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/DimDate.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/DimDate\")","submissionTime":"2023-07-01T17:47:40.917GMT","completionTime":"2023-07-01T17:47:40.994GMT","stageIds":[77],"jobGroup":"9","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":65536,"rowCount":1,"jobId":49,"name":"load at NativeMethodAccessorImpl.java:0","description":"Job group for statement 9:\n#Thanks to MIM for help with this code\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/DimDate.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/DimDate\")","submissionTime":"2023-07-01T17:47:40.635GMT","completionTime":"2023-07-01T17:47:40.685GMT","stageIds":[76],"jobGroup":"9","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}}],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"e33bd096-1760-42ef-b735-d14941467331"},"text/plain":"StatementMeta(, 3c166b9a-f167-4f97-8747-8e1fdca2eefa, 9, Finished, Available)"},"metadata":{}}],"execution_count":7,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}}},{"cell_type":"code","source":["#V order thing you can ignore those two lines\r\n","spark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\r\n","spark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\r\n","#Load from the default lakehouse, make sure you click on the pin <=============\r\n","from pyspark.sql.types import *\r\n","df = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/DimDepartmentGroup.csv\")\r\n","df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/DimDepartmentGroup\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"3c166b9a-f167-4f97-8747-8e1fdca2eefa","statement_id":10,"state":"finished","livy_statement_state":"available","queued_time":"2023-07-01T17:47:27.8396508Z","session_start_time":null,"execution_start_time":"2023-07-01T17:47:43.5798004Z","execution_finish_time":"2023-07-01T17:47:46.178102Z","spark_jobs":{"numbers":{"RUNNING":0,"FAILED":0,"SUCCEEDED":7,"UNKNOWN":0},"jobs":[{"dataWritten":0,"dataRead":4337,"rowCount":50,"jobId":62,"name":"toString at String.java:2994","description":"Delta: Job group for statement 10:\n#V order thing you can ignore those two lines\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/DimDepartmentGroup.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/DimDepartmentGroup\"): Compute snapshot for version: 0","submissionTime":"2023-07-01T17:47:45.351GMT","completionTime":"2023-07-01T17:47:45.378GMT","stageIds":[93,94,95],"jobGroup":"10","status":"SUCCEEDED","numTasks":52,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":51,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":2,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":4337,"dataRead":1523,"rowCount":54,"jobId":61,"name":"toString at String.java:2994","description":"Delta: Job group for statement 10:\n#V order thing you can ignore those two lines\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/DimDepartmentGroup.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/DimDepartmentGroup\"): Compute snapshot for version: 0","submissionTime":"2023-07-01T17:47:44.928GMT","completionTime":"2023-07-01T17:47:45.338GMT","stageIds":[91,92],"jobGroup":"10","status":"SUCCEEDED","numTasks":51,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":1,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":1523,"dataRead":1522,"rowCount":8,"jobId":60,"name":"toString at String.java:2994","description":"Delta: Job group for statement 10:\n#V order thing you can ignore those two lines\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/DimDepartmentGroup.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/DimDepartmentGroup\"): Compute snapshot for version: 0","submissionTime":"2023-07-01T17:47:44.700GMT","completionTime":"2023-07-01T17:47:44.743GMT","stageIds":[90],"jobGroup":"10","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":0,"rowCount":0,"jobId":59,"name":"","description":"Job group for statement 10:\n#V order thing you can ignore those two lines\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/DimDepartmentGroup.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/DimDepartmentGroup\")","submissionTime":"2023-07-01T17:47:44.306GMT","completionTime":"2023-07-01T17:47:44.306GMT","stageIds":[],"jobGroup":"10","status":"SUCCEEDED","numTasks":0,"numActiveTasks":0,"numCompletedTasks":0,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":0,"numActiveStages":0,"numCompletedStages":0,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":1988,"dataRead":362,"rowCount":14,"jobId":58,"name":"save at NativeMethodAccessorImpl.java:0","description":"Job group for statement 10:\n#V order thing you can ignore those two lines\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/DimDepartmentGroup.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/DimDepartmentGroup\")","submissionTime":"2023-07-01T17:47:43.979GMT","completionTime":"2023-07-01T17:47:44.216GMT","stageIds":[88,89],"jobGroup":"10","status":"SUCCEEDED","numTasks":2,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":1,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":362,"dataRead":248,"rowCount":14,"jobId":57,"name":"save at NativeMethodAccessorImpl.java:0","description":"Job group for statement 10:\n#V order thing you can ignore those two lines\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/DimDepartmentGroup.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/DimDepartmentGroup\")","submissionTime":"2023-07-01T17:47:43.910GMT","completionTime":"2023-07-01T17:47:43.945GMT","stageIds":[87],"jobGroup":"10","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":248,"rowCount":1,"jobId":56,"name":"load at NativeMethodAccessorImpl.java:0","description":"Job group for statement 10:\n#V order thing you can ignore those two lines\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/DimDepartmentGroup.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/DimDepartmentGroup\")","submissionTime":"2023-07-01T17:47:43.703GMT","completionTime":"2023-07-01T17:47:43.736GMT","stageIds":[86],"jobGroup":"10","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}}],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"93971b38-deaa-4ca7-9fc0-6bfab6e0a2ea"},"text/plain":"StatementMeta(, 3c166b9a-f167-4f97-8747-8e1fdca2eefa, 10, Finished, Available)"},"metadata":{}}],"execution_count":8,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}}},{"cell_type":"code","source":["#V order thing you can ignore those two lines\r\n","spark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\r\n","spark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\r\n","#Load from the default lakehouse, make sure you click on the pin <=============\r\n","from pyspark.sql.types import *\r\n","df = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/DimEmployee.csv\")\r\n","df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/DimEmployee\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"3c166b9a-f167-4f97-8747-8e1fdca2eefa","statement_id":11,"state":"finished","livy_statement_state":"available","queued_time":"2023-07-01T17:47:28.0239909Z","session_start_time":null,"execution_start_time":"2023-07-01T17:47:46.5189946Z","execution_finish_time":"2023-07-01T17:47:49.2232153Z","spark_jobs":{"numbers":{"RUNNING":0,"FAILED":0,"SUCCEEDED":7,"UNKNOWN":0},"jobs":[{"dataWritten":0,"dataRead":4651,"rowCount":50,"jobId":69,"name":"toString at String.java:2994","description":"Delta: Job group for statement 11:\n#V order thing you can ignore those two lines\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/DimEmployee.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/DimEmployee\"): Compute snapshot for version: 0","submissionTime":"2023-07-01T17:47:48.336GMT","completionTime":"2023-07-01T17:47:48.361GMT","stageIds":[103,104,105],"jobGroup":"11","status":"SUCCEEDED","numTasks":52,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":51,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":2,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":4651,"dataRead":2769,"rowCount":54,"jobId":68,"name":"toString at String.java:2994","description":"Delta: Job group for statement 11:\n#V order thing you can ignore those two lines\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/DimEmployee.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/DimEmployee\"): Compute snapshot for version: 0","submissionTime":"2023-07-01T17:47:47.927GMT","completionTime":"2023-07-01T17:47:48.316GMT","stageIds":[102,101],"jobGroup":"11","status":"SUCCEEDED","numTasks":51,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":1,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":2769,"dataRead":5866,"rowCount":8,"jobId":67,"name":"toString at String.java:2994","description":"Delta: Job group for statement 11:\n#V order thing you can ignore those two lines\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/DimEmployee.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/DimEmployee\"): Compute snapshot for version: 0","submissionTime":"2023-07-01T17:47:47.772GMT","completionTime":"2023-07-01T17:47:47.815GMT","stageIds":[100],"jobGroup":"11","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":0,"rowCount":0,"jobId":66,"name":"","description":"Job group for statement 11:\n#V order thing you can ignore those two lines\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/DimEmployee.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/DimEmployee\")","submissionTime":"2023-07-01T17:47:47.333GMT","completionTime":"2023-07-01T17:47:47.333GMT","stageIds":[],"jobGroup":"11","status":"SUCCEEDED","numTasks":0,"numActiveTasks":0,"numCompletedTasks":0,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":0,"numActiveStages":0,"numCompletedStages":0,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":51397,"dataRead":58618,"rowCount":592,"jobId":65,"name":"save at NativeMethodAccessorImpl.java:0","description":"Job group for statement 11:\n#V order thing you can ignore those two lines\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/DimEmployee.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/DimEmployee\")","submissionTime":"2023-07-01T17:47:46.922GMT","completionTime":"2023-07-01T17:47:47.240GMT","stageIds":[99,98],"jobGroup":"11","status":"SUCCEEDED","numTasks":2,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":1,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":58618,"dataRead":80808,"rowCount":592,"jobId":64,"name":"save at NativeMethodAccessorImpl.java:0","description":"Job group for statement 11:\n#V order thing you can ignore those two lines\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/DimEmployee.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/DimEmployee\")","submissionTime":"2023-07-01T17:47:46.830GMT","completionTime":"2023-07-01T17:47:46.887GMT","stageIds":[97],"jobGroup":"11","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":65536,"rowCount":1,"jobId":63,"name":"load at NativeMethodAccessorImpl.java:0","description":"Job group for statement 11:\n#V order thing you can ignore those two lines\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/DimEmployee.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/DimEmployee\")","submissionTime":"2023-07-01T17:47:46.580GMT","completionTime":"2023-07-01T17:47:46.615GMT","stageIds":[96],"jobGroup":"11","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}}],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"9f29e8d9-0e52-4321-88fd-b58b26d07e6f"},"text/plain":"StatementMeta(, 3c166b9a-f167-4f97-8747-8e1fdca2eefa, 11, Finished, Available)"},"metadata":{}}],"execution_count":9,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}}},{"cell_type":"code","source":["#Thanks to MIM for help with this code\r\n","spark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\r\n","spark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\r\n","#Load from the default lakehouse, make sure you click on the pin <=============\r\n","from pyspark.sql.types import *\r\n","df = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/DimGeography.csv\")\r\n","df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/DimGeography\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"3c166b9a-f167-4f97-8747-8e1fdca2eefa","statement_id":12,"state":"finished","livy_statement_state":"available","queued_time":"2023-07-01T17:47:28.21954Z","session_start_time":null,"execution_start_time":"2023-07-01T17:47:49.5620362Z","execution_finish_time":"2023-07-01T17:47:52.2200846Z","spark_jobs":{"numbers":{"RUNNING":0,"FAILED":0,"SUCCEEDED":7,"UNKNOWN":0},"jobs":[{"dataWritten":0,"dataRead":4444,"rowCount":50,"jobId":76,"name":"toString at String.java:2994","description":"Delta: Job group for statement 12:\n#Thanks to MIM for help with this code\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/DimGeography.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/DimGeography\"): Compute snapshot for version: 0","submissionTime":"2023-07-01T17:47:51.392GMT","completionTime":"2023-07-01T17:47:51.416GMT","stageIds":[114,115,113],"jobGroup":"12","status":"SUCCEEDED","numTasks":52,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":51,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":2,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":4444,"dataRead":1932,"rowCount":54,"jobId":75,"name":"toString at String.java:2994","description":"Delta: Job group for statement 12:\n#Thanks to MIM for help with this code\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/DimGeography.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/DimGeography\"): Compute snapshot for version: 0","submissionTime":"2023-07-01T17:47:50.988GMT","completionTime":"2023-07-01T17:47:51.379GMT","stageIds":[111,112],"jobGroup":"12","status":"SUCCEEDED","numTasks":51,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":1,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":1932,"dataRead":2883,"rowCount":8,"jobId":74,"name":"toString at String.java:2994","description":"Delta: Job group for statement 12:\n#Thanks to MIM for help with this code\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/DimGeography.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/DimGeography\"): Compute snapshot for version: 0","submissionTime":"2023-07-01T17:47:50.829GMT","completionTime":"2023-07-01T17:47:50.874GMT","stageIds":[110],"jobGroup":"12","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":0,"rowCount":0,"jobId":73,"name":"","description":"Job group for statement 12:\n#Thanks to MIM for help with this code\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/DimGeography.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/DimGeography\")","submissionTime":"2023-07-01T17:47:50.341GMT","completionTime":"2023-07-01T17:47:50.341GMT","stageIds":[],"jobGroup":"12","status":"SUCCEEDED","numTasks":0,"numActiveTasks":0,"numCompletedTasks":0,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":0,"numActiveStages":0,"numCompletedStages":0,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":26109,"dataRead":25554,"rowCount":1310,"jobId":72,"name":"save at NativeMethodAccessorImpl.java:0","description":"Job group for statement 12:\n#Thanks to MIM for help with this code\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/DimGeography.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/DimGeography\")","submissionTime":"2023-07-01T17:47:49.962GMT","completionTime":"2023-07-01T17:47:50.230GMT","stageIds":[108,109],"jobGroup":"12","status":"SUCCEEDED","numTasks":2,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":1,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":25554,"dataRead":56888,"rowCount":1310,"jobId":71,"name":"save at NativeMethodAccessorImpl.java:0","description":"Job group for statement 12:\n#Thanks to MIM for help with this code\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/DimGeography.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/DimGeography\")","submissionTime":"2023-07-01T17:47:49.855GMT","completionTime":"2023-07-01T17:47:49.929GMT","stageIds":[107],"jobGroup":"12","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":56888,"rowCount":1,"jobId":70,"name":"load at NativeMethodAccessorImpl.java:0","description":"Job group for statement 12:\n#Thanks to MIM for help with this code\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/DimGeography.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/DimGeography\")","submissionTime":"2023-07-01T17:47:49.628GMT","completionTime":"2023-07-01T17:47:49.661GMT","stageIds":[106],"jobGroup":"12","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}}],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"466fd06c-abc4-4b7b-9e87-652c7c693f47"},"text/plain":"StatementMeta(, 3c166b9a-f167-4f97-8747-8e1fdca2eefa, 12, Finished, Available)"},"metadata":{}}],"execution_count":10,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}}},{"cell_type":"code","source":["#V order thing you can ignore those two lines\r\n","spark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\r\n","spark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\r\n","#Load from the default lakehouse, make sure you click on the pin <=============\r\n","from pyspark.sql.types import *\r\n","df = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/DimOrganization.csv\")\r\n","df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/DimOrganization\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"3c166b9a-f167-4f97-8747-8e1fdca2eefa","statement_id":13,"state":"finished","livy_statement_state":"available","queued_time":"2023-07-01T17:47:28.3405615Z","session_start_time":null,"execution_start_time":"2023-07-01T17:47:52.6004135Z","execution_finish_time":"2023-07-01T17:47:55.3509429Z","spark_jobs":{"numbers":{"RUNNING":0,"FAILED":0,"SUCCEEDED":7,"UNKNOWN":0},"jobs":[{"dataWritten":0,"dataRead":4371,"rowCount":50,"jobId":83,"name":"toString at String.java:2994","description":"Delta: Job group for statement 13:\n#V order thing you can ignore those two lines\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/DimOrganization.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/DimOrganization\"): Compute snapshot for version: 0","submissionTime":"2023-07-01T17:47:54.230GMT","completionTime":"2023-07-01T17:47:54.254GMT","stageIds":[125,123,124],"jobGroup":"13","status":"SUCCEEDED","numTasks":52,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":51,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":2,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":4371,"dataRead":1616,"rowCount":54,"jobId":82,"name":"toString at String.java:2994","description":"Delta: Job group for statement 13:\n#V order thing you can ignore those two lines\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/DimOrganization.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/DimOrganization\"): Compute snapshot for version: 0","submissionTime":"2023-07-01T17:47:53.803GMT","completionTime":"2023-07-01T17:47:54.216GMT","stageIds":[121,122],"jobGroup":"13","status":"SUCCEEDED","numTasks":51,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":1,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":1616,"dataRead":1824,"rowCount":8,"jobId":81,"name":"toString at String.java:2994","description":"Delta: Job group for statement 13:\n#V order thing you can ignore those two lines\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/DimOrganization.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/DimOrganization\"): Compute snapshot for version: 0","submissionTime":"2023-07-01T17:47:53.655GMT","completionTime":"2023-07-01T17:47:53.699GMT","stageIds":[120],"jobGroup":"13","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":0,"rowCount":0,"jobId":80,"name":"","description":"Job group for statement 13:\n#V order thing you can ignore those two lines\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/DimOrganization.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/DimOrganization\")","submissionTime":"2023-07-01T17:47:53.254GMT","completionTime":"2023-07-01T17:47:53.254GMT","stageIds":[],"jobGroup":"13","status":"SUCCEEDED","numTasks":0,"numActiveTasks":0,"numCompletedTasks":0,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":0,"numActiveStages":0,"numCompletedStages":0,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":3007,"dataRead":642,"rowCount":28,"jobId":79,"name":"save at NativeMethodAccessorImpl.java:0","description":"Job group for statement 13:\n#V order thing you can ignore those two lines\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/DimOrganization.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/DimOrganization\")","submissionTime":"2023-07-01T17:47:52.966GMT","completionTime":"2023-07-01T17:47:53.171GMT","stageIds":[118,119],"jobGroup":"13","status":"SUCCEEDED","numTasks":2,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":1,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":642,"dataRead":496,"rowCount":28,"jobId":78,"name":"save at NativeMethodAccessorImpl.java:0","description":"Job group for statement 13:\n#V order thing you can ignore those two lines\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/DimOrganization.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/DimOrganization\")","submissionTime":"2023-07-01T17:47:52.889GMT","completionTime":"2023-07-01T17:47:52.929GMT","stageIds":[117],"jobGroup":"13","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":496,"rowCount":1,"jobId":77,"name":"load at NativeMethodAccessorImpl.java:0","description":"Job group for statement 13:\n#V order thing you can ignore those two lines\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/DimOrganization.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/DimOrganization\")","submissionTime":"2023-07-01T17:47:52.677GMT","completionTime":"2023-07-01T17:47:52.710GMT","stageIds":[116],"jobGroup":"13","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}}],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"c48047de-3f82-4dd2-9e49-f4e3ba6aa8f1"},"text/plain":"StatementMeta(, 3c166b9a-f167-4f97-8747-8e1fdca2eefa, 13, Finished, Available)"},"metadata":{}}],"execution_count":11,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}}},{"cell_type":"code","source":["#Thanks to MIM for help with this code\r\n","spark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\r\n","spark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\r\n","#Load from the default lakehouse, make sure you click on the pin <=============\r\n","from pyspark.sql.types import *\r\n","df = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/DimProduct.csv\")\r\n","df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/DimProduct\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"3c166b9a-f167-4f97-8747-8e1fdca2eefa","statement_id":14,"state":"finished","livy_statement_state":"available","queued_time":"2023-07-01T17:47:28.4278758Z","session_start_time":null,"execution_start_time":"2023-07-01T17:47:55.7104525Z","execution_finish_time":"2023-07-01T17:47:58.473175Z","spark_jobs":{"numbers":{"RUNNING":0,"FAILED":0,"SUCCEEDED":7,"UNKNOWN":0},"jobs":[{"dataWritten":0,"dataRead":4722,"rowCount":50,"jobId":90,"name":"toString at String.java:2994","description":"Delta: Job group for statement 14:\n#Thanks to MIM for help with this code\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/DimProduct.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/DimProduct\"): Compute snapshot for version: 0","submissionTime":"2023-07-01T17:47:57.566GMT","completionTime":"2023-07-01T17:47:57.591GMT","stageIds":[135,133,134],"jobGroup":"14","status":"SUCCEEDED","numTasks":52,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":51,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":2,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":4722,"dataRead":3260,"rowCount":54,"jobId":89,"name":"toString at String.java:2994","description":"Delta: Job group for statement 14:\n#Thanks to MIM for help with this code\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/DimProduct.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/DimProduct\"): Compute snapshot for version: 0","submissionTime":"2023-07-01T17:47:57.176GMT","completionTime":"2023-07-01T17:47:57.550GMT","stageIds":[132,131],"jobGroup":"14","status":"SUCCEEDED","numTasks":51,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":1,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":3260,"dataRead":6957,"rowCount":8,"jobId":88,"name":"toString at String.java:2994","description":"Delta: Job group for statement 14:\n#Thanks to MIM for help with this code\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/DimProduct.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/DimProduct\"): Compute snapshot for version: 0","submissionTime":"2023-07-01T17:47:56.963GMT","completionTime":"2023-07-01T17:47:57.007GMT","stageIds":[130],"jobGroup":"14","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":0,"rowCount":0,"jobId":87,"name":"","description":"Job group for statement 14:\n#Thanks to MIM for help with this code\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/DimProduct.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/DimProduct\")","submissionTime":"2023-07-01T17:47:56.574GMT","completionTime":"2023-07-01T17:47:56.574GMT","stageIds":[],"jobGroup":"14","status":"SUCCEEDED","numTasks":0,"numActiveTasks":0,"numCompletedTasks":0,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":0,"numActiveStages":0,"numCompletedStages":0,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":80705,"dataRead":89196,"rowCount":1212,"jobId":86,"name":"save at NativeMethodAccessorImpl.java:0","description":"Job group for statement 14:\n#Thanks to MIM for help with this code\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/DimProduct.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/DimProduct\")","submissionTime":"2023-07-01T17:47:56.165GMT","completionTime":"2023-07-01T17:47:56.480GMT","stageIds":[129,128],"jobGroup":"14","status":"SUCCEEDED","numTasks":2,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":1,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":89196,"dataRead":525766,"rowCount":1212,"jobId":85,"name":"save at NativeMethodAccessorImpl.java:0","description":"Job group for statement 14:\n#Thanks to MIM for help with this code\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/DimProduct.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/DimProduct\")","submissionTime":"2023-07-01T17:47:56.063GMT","completionTime":"2023-07-01T17:47:56.131GMT","stageIds":[127],"jobGroup":"14","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":65536,"rowCount":1,"jobId":84,"name":"load at NativeMethodAccessorImpl.java:0","description":"Job group for statement 14:\n#Thanks to MIM for help with this code\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/DimProduct.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/DimProduct\")","submissionTime":"2023-07-01T17:47:55.795GMT","completionTime":"2023-07-01T17:47:55.849GMT","stageIds":[126],"jobGroup":"14","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}}],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"53b4c992-32ed-4e3b-b8b2-8b5d9ebd5f56"},"text/plain":"StatementMeta(, 3c166b9a-f167-4f97-8747-8e1fdca2eefa, 14, Finished, Available)"},"metadata":{}}],"execution_count":12,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"advisor":{"adviceMetadata":"{\"artifactId\":\"4b6dc142-53a6-4302-92d0-b0982019ff67\",\"activityId\":\"3c166b9a-f167-4f97-8747-8e1fdca2eefa\",\"applicationId\":\"application_1688218711538_0001\",\"jobGroupId\":\"14\",\"advices\":{\"info\":1}}"}}},{"cell_type":"code","source":["#V order thing you can ignore those two lines\r\n","spark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\r\n","spark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\r\n","#Load from the default lakehouse, make sure you click on the pin <=============\r\n","from pyspark.sql.types import *\r\n","df = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/DimProductCategory.csv\")\r\n","df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/DimProductCategory\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"3c166b9a-f167-4f97-8747-8e1fdca2eefa","statement_id":15,"state":"finished","livy_statement_state":"available","queued_time":"2023-07-01T17:47:28.5237058Z","session_start_time":null,"execution_start_time":"2023-07-01T17:47:58.821113Z","execution_finish_time":"2023-07-01T17:48:01.5203815Z","spark_jobs":{"numbers":{"RUNNING":0,"FAILED":0,"SUCCEEDED":7,"UNKNOWN":0},"jobs":[{"dataWritten":0,"dataRead":4365,"rowCount":50,"jobId":97,"name":"toString at String.java:2994","description":"Delta: Job group for statement 15:\n#V order thing you can ignore those two lines\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/DimProductCategory.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/DimProductCategory\"): Compute snapshot for version: 0","submissionTime":"2023-07-01T17:48:00.413GMT","completionTime":"2023-07-01T17:48:00.438GMT","stageIds":[143,144,145],"jobGroup":"15","status":"SUCCEEDED","numTasks":52,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":51,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":2,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":4365,"dataRead":1615,"rowCount":54,"jobId":96,"name":"toString at String.java:2994","description":"Delta: Job group for statement 15:\n#V order thing you can ignore those two lines\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/DimProductCategory.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/DimProductCategory\"): Compute snapshot for version: 0","submissionTime":"2023-07-01T17:48:00.024GMT","completionTime":"2023-07-01T17:48:00.396GMT","stageIds":[141,142],"jobGroup":"15","status":"SUCCEEDED","numTasks":51,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":1,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":1615,"dataRead":1986,"rowCount":8,"jobId":95,"name":"toString at String.java:2994","description":"Delta: Job group for statement 15:\n#V order thing you can ignore those two lines\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/DimProductCategory.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/DimProductCategory\"): Compute snapshot for version: 0","submissionTime":"2023-07-01T17:47:59.875GMT","completionTime":"2023-07-01T17:47:59.922GMT","stageIds":[140],"jobGroup":"15","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":0,"rowCount":0,"jobId":94,"name":"","description":"Job group for statement 15:\n#V order thing you can ignore those two lines\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/DimProductCategory.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/DimProductCategory\")","submissionTime":"2023-07-01T17:47:59.447GMT","completionTime":"2023-07-01T17:47:59.447GMT","stageIds":[],"jobGroup":"15","status":"SUCCEEDED","numTasks":0,"numActiveTasks":0,"numCompletedTasks":0,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":0,"numActiveStages":0,"numCompletedStages":0,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":3015,"dataRead":299,"rowCount":8,"jobId":93,"name":"save at NativeMethodAccessorImpl.java:0","description":"Job group for statement 15:\n#V order thing you can ignore those two lines\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/DimProductCategory.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/DimProductCategory\")","submissionTime":"2023-07-01T17:47:59.170GMT","completionTime":"2023-07-01T17:47:59.369GMT","stageIds":[139,138],"jobGroup":"15","status":"SUCCEEDED","numTasks":2,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":1,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":299,"dataRead":260,"rowCount":8,"jobId":92,"name":"save at NativeMethodAccessorImpl.java:0","description":"Job group for statement 15:\n#V order thing you can ignore those two lines\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/DimProductCategory.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/DimProductCategory\")","submissionTime":"2023-07-01T17:47:59.103GMT","completionTime":"2023-07-01T17:47:59.136GMT","stageIds":[137],"jobGroup":"15","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":260,"rowCount":1,"jobId":91,"name":"load at NativeMethodAccessorImpl.java:0","description":"Job group for statement 15:\n#V order thing you can ignore those two lines\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/DimProductCategory.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/DimProductCategory\")","submissionTime":"2023-07-01T17:47:58.892GMT","completionTime":"2023-07-01T17:47:58.930GMT","stageIds":[136],"jobGroup":"15","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}}],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"87c06c9c-f7e5-4338-8dea-19a3a6680a06"},"text/plain":"StatementMeta(, 3c166b9a-f167-4f97-8747-8e1fdca2eefa, 15, Finished, Available)"},"metadata":{}}],"execution_count":13,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}}},{"cell_type":"code","source":["#Thanks to MIM for help with this code\r\n","spark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\r\n","spark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\r\n","#Load from the default lakehouse, make sure you click on the pin <=============\r\n","from pyspark.sql.types import *\r\n","df = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/DimProductSubcategory.csv\")\r\n","df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/DimProductSubcategory\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"3c166b9a-f167-4f97-8747-8e1fdca2eefa","statement_id":16,"state":"finished","livy_statement_state":"available","queued_time":"2023-07-01T17:47:28.6202842Z","session_start_time":null,"execution_start_time":"2023-07-01T17:48:01.8849015Z","execution_finish_time":"2023-07-01T17:48:04.5741203Z","spark_jobs":{"numbers":{"RUNNING":0,"FAILED":0,"SUCCEEDED":7,"UNKNOWN":0},"jobs":[{"dataWritten":0,"dataRead":4376,"rowCount":50,"jobId":104,"name":"toString at String.java:2994","description":"Delta: Job group for statement 16:\n#Thanks to MIM for help with this code\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/DimProductSubcategory.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/DimProductSubcategory\"): Compute snapshot for version: 0","submissionTime":"2023-07-01T17:48:03.530GMT","completionTime":"2023-07-01T17:48:03.555GMT","stageIds":[153,154,155],"jobGroup":"16","status":"SUCCEEDED","numTasks":52,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":51,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":2,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":4376,"dataRead":1679,"rowCount":54,"jobId":103,"name":"toString at String.java:2994","description":"Delta: Job group for statement 16:\n#Thanks to MIM for help with this code\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/DimProductSubcategory.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/DimProductSubcategory\"): Compute snapshot for version: 0","submissionTime":"2023-07-01T17:48:03.159GMT","completionTime":"2023-07-01T17:48:03.517GMT","stageIds":[151,152],"jobGroup":"16","status":"SUCCEEDED","numTasks":51,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":1,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":1679,"dataRead":2224,"rowCount":8,"jobId":102,"name":"toString at String.java:2994","description":"Delta: Job group for statement 16:\n#Thanks to MIM for help with this code\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/DimProductSubcategory.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/DimProductSubcategory\"): Compute snapshot for version: 0","submissionTime":"2023-07-01T17:48:03.006GMT","completionTime":"2023-07-01T17:48:03.048GMT","stageIds":[150],"jobGroup":"16","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":0,"rowCount":0,"jobId":101,"name":"","description":"Job group for statement 16:\n#Thanks to MIM for help with this code\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/DimProductSubcategory.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/DimProductSubcategory\")","submissionTime":"2023-07-01T17:48:02.532GMT","completionTime":"2023-07-01T17:48:02.532GMT","stageIds":[],"jobGroup":"16","status":"SUCCEEDED","numTasks":0,"numActiveTasks":0,"numCompletedTasks":0,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":0,"numActiveStages":0,"numCompletedStages":0,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":5111,"dataRead":2499,"rowCount":74,"jobId":100,"name":"save at NativeMethodAccessorImpl.java:0","description":"Job group for statement 16:\n#Thanks to MIM for help with this code\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/DimProductSubcategory.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/DimProductSubcategory\")","submissionTime":"2023-07-01T17:48:02.229GMT","completionTime":"2023-07-01T17:48:02.435GMT","stageIds":[148,149],"jobGroup":"16","status":"SUCCEEDED","numTasks":2,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":1,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":2499,"dataRead":1724,"rowCount":74,"jobId":99,"name":"save at NativeMethodAccessorImpl.java:0","description":"Job group for statement 16:\n#Thanks to MIM for help with this code\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/DimProductSubcategory.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/DimProductSubcategory\")","submissionTime":"2023-07-01T17:48:02.154GMT","completionTime":"2023-07-01T17:48:02.196GMT","stageIds":[147],"jobGroup":"16","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":1724,"rowCount":1,"jobId":98,"name":"load at NativeMethodAccessorImpl.java:0","description":"Job group for statement 16:\n#Thanks to MIM for help with this code\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/DimProductSubcategory.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/DimProductSubcategory\")","submissionTime":"2023-07-01T17:48:01.943GMT","completionTime":"2023-07-01T17:48:01.976GMT","stageIds":[146],"jobGroup":"16","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}}],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"3fe740c3-6932-45f7-8e2f-7dd4b38af01d"},"text/plain":"StatementMeta(, 3c166b9a-f167-4f97-8747-8e1fdca2eefa, 16, Finished, Available)"},"metadata":{}}],"execution_count":14,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}}},{"cell_type":"code","source":["#V order thing you can ignore those two lines\r\n","spark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\r\n","spark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\r\n","#Load from the default lakehouse, make sure you click on the pin <=============\r\n","from pyspark.sql.types import *\r\n","df = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/DimPromotion.csv\")\r\n","df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/DimPromotion\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"3c166b9a-f167-4f97-8747-8e1fdca2eefa","statement_id":17,"state":"finished","livy_statement_state":"available","queued_time":"2023-07-01T17:47:28.708058Z","session_start_time":null,"execution_start_time":"2023-07-01T17:48:04.9522909Z","execution_finish_time":"2023-07-01T17:48:07.6143457Z","spark_jobs":{"numbers":{"RUNNING":0,"FAILED":0,"SUCCEEDED":7,"UNKNOWN":0},"jobs":[{"dataWritten":0,"dataRead":4464,"rowCount":50,"jobId":111,"name":"toString at String.java:2994","description":"Delta: Job group for statement 17:\n#V order thing you can ignore those two lines\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/DimPromotion.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/DimPromotion\"): Compute snapshot for version: 0","submissionTime":"2023-07-01T17:48:06.554GMT","completionTime":"2023-07-01T17:48:06.579GMT","stageIds":[165,163,164],"jobGroup":"17","status":"SUCCEEDED","numTasks":52,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":51,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":2,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":4464,"dataRead":2201,"rowCount":54,"jobId":110,"name":"toString at String.java:2994","description":"Delta: Job group for statement 17:\n#V order thing you can ignore those two lines\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/DimPromotion.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/DimPromotion\"): Compute snapshot for version: 0","submissionTime":"2023-07-01T17:48:06.209GMT","completionTime":"2023-07-01T17:48:06.541GMT","stageIds":[161,162],"jobGroup":"17","status":"SUCCEEDED","numTasks":51,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":1,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":2201,"dataRead":4010,"rowCount":8,"jobId":109,"name":"toString at String.java:2994","description":"Delta: Job group for statement 17:\n#V order thing you can ignore those two lines\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/DimPromotion.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/DimPromotion\"): Compute snapshot for version: 0","submissionTime":"2023-07-01T17:48:06.061GMT","completionTime":"2023-07-01T17:48:06.102GMT","stageIds":[160],"jobGroup":"17","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":0,"rowCount":0,"jobId":108,"name":"","description":"Job group for statement 17:\n#V order thing you can ignore those two lines\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/DimPromotion.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/DimPromotion\")","submissionTime":"2023-07-01T17:48:05.671GMT","completionTime":"2023-07-01T17:48:05.671GMT","stageIds":[],"jobGroup":"17","status":"SUCCEEDED","numTasks":0,"numActiveTasks":0,"numCompletedTasks":0,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":0,"numActiveStages":0,"numCompletedStages":0,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":11107,"dataRead":2730,"rowCount":32,"jobId":107,"name":"save at NativeMethodAccessorImpl.java:0","description":"Job group for statement 17:\n#V order thing you can ignore those two lines\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/DimPromotion.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/DimPromotion\")","submissionTime":"2023-07-01T17:48:05.349GMT","completionTime":"2023-07-01T17:48:05.572GMT","stageIds":[158,159],"jobGroup":"17","status":"SUCCEEDED","numTasks":2,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":1,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":2730,"dataRead":4286,"rowCount":32,"jobId":106,"name":"save at NativeMethodAccessorImpl.java:0","description":"Job group for statement 17:\n#V order thing you can ignore those two lines\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/DimPromotion.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/DimPromotion\")","submissionTime":"2023-07-01T17:48:05.234GMT","completionTime":"2023-07-01T17:48:05.276GMT","stageIds":[157],"jobGroup":"17","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":4286,"rowCount":1,"jobId":105,"name":"load at NativeMethodAccessorImpl.java:0","description":"Job group for statement 17:\n#V order thing you can ignore those two lines\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/DimPromotion.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/DimPromotion\")","submissionTime":"2023-07-01T17:48:05.015GMT","completionTime":"2023-07-01T17:48:05.046GMT","stageIds":[156],"jobGroup":"17","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}}],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"a1bf7040-25e7-4770-8712-863f00c2fb0f"},"text/plain":"StatementMeta(, 3c166b9a-f167-4f97-8747-8e1fdca2eefa, 17, Finished, Available)"},"metadata":{}}],"execution_count":15,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"advisor":{"adviceMetadata":"{\"artifactId\":\"4b6dc142-53a6-4302-92d0-b0982019ff67\",\"activityId\":\"3c166b9a-f167-4f97-8747-8e1fdca2eefa\",\"applicationId\":\"application_1688218711538_0001\",\"jobGroupId\":\"17\",\"advices\":{\"info\":1}}"}}},{"cell_type":"code","source":["#V order thing you can ignore those two lines\r\n","spark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\r\n","spark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\r\n","#Load from the default lakehouse, make sure you click on the pin <=============\r\n","from pyspark.sql.types import *\r\n","df = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/DimReseller.csv\")\r\n","df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/DimReseller\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"3c166b9a-f167-4f97-8747-8e1fdca2eefa","statement_id":18,"state":"finished","livy_statement_state":"available","queued_time":"2023-07-01T17:47:28.8036909Z","session_start_time":null,"execution_start_time":"2023-07-01T17:48:07.965066Z","execution_finish_time":"2023-07-01T17:48:10.6110192Z","spark_jobs":{"numbers":{"RUNNING":0,"FAILED":0,"SUCCEEDED":7,"UNKNOWN":0},"jobs":[{"dataWritten":0,"dataRead":4558,"rowCount":50,"jobId":118,"name":"toString at String.java:2994","description":"Delta: Job group for statement 18:\n#V order thing you can ignore those two lines\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/DimReseller.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/DimReseller\"): Compute snapshot for version: 0","submissionTime":"2023-07-01T17:48:09.621GMT","completionTime":"2023-07-01T17:48:09.643GMT","stageIds":[173,174,175],"jobGroup":"18","status":"SUCCEEDED","numTasks":52,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":51,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":2,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":4558,"dataRead":2378,"rowCount":54,"jobId":117,"name":"toString at String.java:2994","description":"Delta: Job group for statement 18:\n#V order thing you can ignore those two lines\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/DimReseller.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/DimReseller\"): Compute snapshot for version: 0","submissionTime":"2023-07-01T17:48:09.272GMT","completionTime":"2023-07-01T17:48:09.609GMT","stageIds":[171,172],"jobGroup":"18","status":"SUCCEEDED","numTasks":51,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":1,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":2378,"dataRead":4205,"rowCount":8,"jobId":116,"name":"toString at String.java:2994","description":"Delta: Job group for statement 18:\n#V order thing you can ignore those two lines\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/DimReseller.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/DimReseller\"): Compute snapshot for version: 0","submissionTime":"2023-07-01T17:48:09.133GMT","completionTime":"2023-07-01T17:48:09.175GMT","stageIds":[170],"jobGroup":"18","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":0,"rowCount":0,"jobId":115,"name":"","description":"Job group for statement 18:\n#V order thing you can ignore those two lines\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/DimReseller.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/DimReseller\")","submissionTime":"2023-07-01T17:48:08.752GMT","completionTime":"2023-07-01T17:48:08.752GMT","stageIds":[],"jobGroup":"18","status":"SUCCEEDED","numTasks":0,"numActiveTasks":0,"numCompletedTasks":0,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":0,"numActiveStages":0,"numCompletedStages":0,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":53315,"dataRead":95860,"rowCount":1402,"jobId":114,"name":"save at NativeMethodAccessorImpl.java:0","description":"Job group for statement 18:\n#V order thing you can ignore those two lines\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/DimReseller.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/DimReseller\")","submissionTime":"2023-07-01T17:48:08.380GMT","completionTime":"2023-07-01T17:48:08.654GMT","stageIds":[168,169],"jobGroup":"18","status":"SUCCEEDED","numTasks":2,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":1,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":95860,"dataRead":126109,"rowCount":1402,"jobId":113,"name":"save at NativeMethodAccessorImpl.java:0","description":"Job group for statement 18:\n#V order thing you can ignore those two lines\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/DimReseller.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/DimReseller\")","submissionTime":"2023-07-01T17:48:08.295GMT","completionTime":"2023-07-01T17:48:08.346GMT","stageIds":[167],"jobGroup":"18","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":65536,"rowCount":1,"jobId":112,"name":"load at NativeMethodAccessorImpl.java:0","description":"Job group for statement 18:\n#V order thing you can ignore those two lines\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/DimReseller.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/DimReseller\")","submissionTime":"2023-07-01T17:48:08.031GMT","completionTime":"2023-07-01T17:48:08.065GMT","stageIds":[166],"jobGroup":"18","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}}],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"391e0bd4-b110-47bb-8785-8b775ad5f89a"},"text/plain":"StatementMeta(, 3c166b9a-f167-4f97-8747-8e1fdca2eefa, 18, Finished, Available)"},"metadata":{}}],"execution_count":16,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"advisor":{"adviceMetadata":"{\"artifactId\":\"4b6dc142-53a6-4302-92d0-b0982019ff67\",\"activityId\":\"3c166b9a-f167-4f97-8747-8e1fdca2eefa\",\"applicationId\":\"application_1688218711538_0001\",\"jobGroupId\":\"18\",\"advices\":{\"info\":1}}"}}},{"cell_type":"code","source":["#Thanks to MIM for help with this code\r\n","spark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\r\n","spark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\r\n","#Load from the default lakehouse, make sure you click on the pin <=============\r\n","from pyspark.sql.types import *\r\n","df = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/DimSalesReason.csv\")\r\n","df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/DimSalesReason\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"3c166b9a-f167-4f97-8747-8e1fdca2eefa","statement_id":19,"state":"finished","livy_statement_state":"available","queued_time":"2023-07-01T17:47:28.9280298Z","session_start_time":null,"execution_start_time":"2023-07-01T17:48:10.9720533Z","execution_finish_time":"2023-07-01T17:48:13.5743285Z","spark_jobs":{"numbers":{"RUNNING":0,"FAILED":0,"SUCCEEDED":7,"UNKNOWN":0},"jobs":[{"dataWritten":0,"dataRead":4345,"rowCount":50,"jobId":125,"name":"toString at String.java:2994","description":"Delta: Job group for statement 19:\n#Thanks to MIM for help with this code\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/DimSalesReason.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/DimSalesReason\"): Compute snapshot for version: 0","submissionTime":"2023-07-01T17:48:12.560GMT","completionTime":"2023-07-01T17:48:12.582GMT","stageIds":[183,184,185],"jobGroup":"19","status":"SUCCEEDED","numTasks":52,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":51,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":2,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":4345,"dataRead":1567,"rowCount":54,"jobId":124,"name":"toString at String.java:2994","description":"Delta: Job group for statement 19:\n#Thanks to MIM for help with this code\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/DimSalesReason.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/DimSalesReason\"): Compute snapshot for version: 0","submissionTime":"2023-07-01T17:48:12.209GMT","completionTime":"2023-07-01T17:48:12.548GMT","stageIds":[181,182],"jobGroup":"19","status":"SUCCEEDED","numTasks":51,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":1,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":1567,"dataRead":1691,"rowCount":8,"jobId":123,"name":"toString at String.java:2994","description":"Delta: Job group for statement 19:\n#Thanks to MIM for help with this code\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/DimSalesReason.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/DimSalesReason\"): Compute snapshot for version: 0","submissionTime":"2023-07-01T17:48:12.070GMT","completionTime":"2023-07-01T17:48:12.110GMT","stageIds":[180],"jobGroup":"19","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":0,"rowCount":0,"jobId":122,"name":"","description":"Job group for statement 19:\n#Thanks to MIM for help with this code\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/DimSalesReason.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/DimSalesReason\")","submissionTime":"2023-07-01T17:48:11.676GMT","completionTime":"2023-07-01T17:48:11.676GMT","stageIds":[],"jobGroup":"19","status":"SUCCEEDED","numTasks":0,"numActiveTasks":0,"numCompletedTasks":0,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":0,"numActiveStages":0,"numCompletedStages":0,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":2550,"dataRead":463,"rowCount":20,"jobId":121,"name":"save at <unknown>:0","description":"Job group for statement 19:\n#Thanks to MIM for help with this code\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/DimSalesReason.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/DimSalesReason\")","submissionTime":"2023-07-01T17:48:11.387GMT","completionTime":"2023-07-01T17:48:11.602GMT","stageIds":[179,178],"jobGroup":"19","status":"SUCCEEDED","numTasks":2,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":1,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":463,"dataRead":335,"rowCount":20,"jobId":120,"name":"save at <unknown>:0","description":"Job group for statement 19:\n#Thanks to MIM for help with this code\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/DimSalesReason.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/DimSalesReason\")","submissionTime":"2023-07-01T17:48:11.304GMT","completionTime":"2023-07-01T17:48:11.344GMT","stageIds":[177],"jobGroup":"19","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":335,"rowCount":1,"jobId":119,"name":"load at <unknown>:0","description":"Job group for statement 19:\n#Thanks to MIM for help with this code\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/DimSalesReason.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/DimSalesReason\")","submissionTime":"2023-07-01T17:48:11.086GMT","completionTime":"2023-07-01T17:48:11.121GMT","stageIds":[176],"jobGroup":"19","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}}],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"902cf195-aaca-4075-a5a5-4b9bb8b96283"},"text/plain":"StatementMeta(, 3c166b9a-f167-4f97-8747-8e1fdca2eefa, 19, Finished, Available)"},"metadata":{}}],"execution_count":17,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}}},{"cell_type":"code","source":["#Thanks to MIM for help with this code\r\n","spark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\r\n","spark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\r\n","#Load from the default lakehouse, make sure you click on the pin <=============\r\n","from pyspark.sql.types import *\r\n","df = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/DimSalesTerritory.csv\")\r\n","df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/DimSalesTerritory\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"3c166b9a-f167-4f97-8747-8e1fdca2eefa","statement_id":20,"state":"finished","livy_statement_state":"available","queued_time":"2023-07-01T17:47:29.0478262Z","session_start_time":null,"execution_start_time":"2023-07-01T17:48:13.9223498Z","execution_finish_time":"2023-07-01T17:48:16.6282198Z","spark_jobs":{"numbers":{"RUNNING":0,"FAILED":0,"SUCCEEDED":7,"UNKNOWN":0},"jobs":[{"dataWritten":0,"dataRead":4367,"rowCount":50,"jobId":132,"name":"toString at String.java:2994","description":"Delta: Job group for statement 20:\n#Thanks to MIM for help with this code\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/DimSalesTerritory.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/DimSalesTerritory\"): Compute snapshot for version: 0","submissionTime":"2023-07-01T17:48:15.462GMT","completionTime":"2023-07-01T17:48:15.483GMT","stageIds":[194,195,193],"jobGroup":"20","status":"SUCCEEDED","numTasks":52,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":51,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":2,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":4367,"dataRead":1679,"rowCount":54,"jobId":131,"name":"toString at String.java:2994","description":"Delta: Job group for statement 20:\n#Thanks to MIM for help with this code\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/DimSalesTerritory.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/DimSalesTerritory\"): Compute snapshot for version: 0","submissionTime":"2023-07-01T17:48:15.117GMT","completionTime":"2023-07-01T17:48:15.446GMT","stageIds":[191,192],"jobGroup":"20","status":"SUCCEEDED","numTasks":51,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":1,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":1679,"dataRead":2122,"rowCount":8,"jobId":130,"name":"toString at String.java:2994","description":"Delta: Job group for statement 20:\n#Thanks to MIM for help with this code\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/DimSalesTerritory.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/DimSalesTerritory\"): Compute snapshot for version: 0","submissionTime":"2023-07-01T17:48:14.961GMT","completionTime":"2023-07-01T17:48:15.003GMT","stageIds":[190],"jobGroup":"20","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":0,"rowCount":0,"jobId":129,"name":"","description":"Job group for statement 20:\n#Thanks to MIM for help with this code\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/DimSalesTerritory.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/DimSalesTerritory\")","submissionTime":"2023-07-01T17:48:14.564GMT","completionTime":"2023-07-01T17:48:14.564GMT","stageIds":[],"jobGroup":"20","status":"SUCCEEDED","numTasks":0,"numActiveTasks":0,"numCompletedTasks":0,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":0,"numActiveStages":0,"numCompletedStages":0,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":3932,"dataRead":782,"rowCount":22,"jobId":128,"name":"save at <unknown>:0","description":"Job group for statement 20:\n#Thanks to MIM for help with this code\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/DimSalesTerritory.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/DimSalesTerritory\")","submissionTime":"2023-07-01T17:48:14.277GMT","completionTime":"2023-07-01T17:48:14.490GMT","stageIds":[188,189],"jobGroup":"20","status":"SUCCEEDED","numTasks":2,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":1,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":782,"dataRead":916,"rowCount":22,"jobId":127,"name":"save at <unknown>:0","description":"Job group for statement 20:\n#Thanks to MIM for help with this code\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/DimSalesTerritory.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/DimSalesTerritory\")","submissionTime":"2023-07-01T17:48:14.208GMT","completionTime":"2023-07-01T17:48:14.243GMT","stageIds":[187],"jobGroup":"20","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":916,"rowCount":1,"jobId":126,"name":"load at <unknown>:0","description":"Job group for statement 20:\n#Thanks to MIM for help with this code\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/DimSalesTerritory.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/DimSalesTerritory\")","submissionTime":"2023-07-01T17:48:14.004GMT","completionTime":"2023-07-01T17:48:14.034GMT","stageIds":[186],"jobGroup":"20","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}}],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"3b4aac46-1ccb-49bc-8419-57b9f98a2646"},"text/plain":"StatementMeta(, 3c166b9a-f167-4f97-8747-8e1fdca2eefa, 20, Finished, Available)"},"metadata":{}}],"execution_count":18,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}}},{"cell_type":"code","source":["#Thanks to MIM for help with this code\r\n","spark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\r\n","spark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\r\n","#Load from the default lakehouse, make sure you click on the pin <=============\r\n","from pyspark.sql.types import *\r\n","df = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/DimScenario.csv\")\r\n","df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/DimScenario\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"3c166b9a-f167-4f97-8747-8e1fdca2eefa","statement_id":21,"state":"finished","livy_statement_state":"available","queued_time":"2023-07-01T17:47:29.1519977Z","session_start_time":null,"execution_start_time":"2023-07-01T17:48:16.9651481Z","execution_finish_time":"2023-07-01T17:48:19.6139817Z","spark_jobs":{"numbers":{"RUNNING":0,"FAILED":0,"SUCCEEDED":7,"UNKNOWN":0},"jobs":[{"dataWritten":0,"dataRead":4316,"rowCount":50,"jobId":139,"name":"toString at String.java:2994","description":"Delta: Job group for statement 21:\n#Thanks to MIM for help with this code\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/DimScenario.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/DimScenario\"): Compute snapshot for version: 0","submissionTime":"2023-07-01T17:48:18.452GMT","completionTime":"2023-07-01T17:48:18.472GMT","stageIds":[204,205,203],"jobGroup":"21","status":"SUCCEEDED","numTasks":52,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":51,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":2,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":4316,"dataRead":1460,"rowCount":54,"jobId":138,"name":"toString at String.java:2994","description":"Delta: Job group for statement 21:\n#Thanks to MIM for help with this code\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/DimScenario.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/DimScenario\"): Compute snapshot for version: 0","submissionTime":"2023-07-01T17:48:18.127GMT","completionTime":"2023-07-01T17:48:18.439GMT","stageIds":[201,202],"jobGroup":"21","status":"SUCCEEDED","numTasks":51,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":1,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":1460,"dataRead":1254,"rowCount":8,"jobId":137,"name":"toString at String.java:2994","description":"Delta: Job group for statement 21:\n#Thanks to MIM for help with this code\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/DimScenario.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/DimScenario\"): Compute snapshot for version: 0","submissionTime":"2023-07-01T17:48:17.967GMT","completionTime":"2023-07-01T17:48:18.009GMT","stageIds":[200],"jobGroup":"21","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":0,"rowCount":0,"jobId":136,"name":"","description":"Job group for statement 21:\n#Thanks to MIM for help with this code\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/DimScenario.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/DimScenario\")","submissionTime":"2023-07-01T17:48:17.587GMT","completionTime":"2023-07-01T17:48:17.587GMT","stageIds":[],"jobGroup":"21","status":"SUCCEEDED","numTasks":0,"numActiveTasks":0,"numCompletedTasks":0,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":0,"numActiveStages":0,"numCompletedStages":0,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":1242,"dataRead":121,"rowCount":6,"jobId":135,"name":"save at <unknown>:0","description":"Job group for statement 21:\n#Thanks to MIM for help with this code\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/DimScenario.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/DimScenario\")","submissionTime":"2023-07-01T17:48:17.317GMT","completionTime":"2023-07-01T17:48:17.508GMT","stageIds":[198,199],"jobGroup":"21","status":"SUCCEEDED","numTasks":2,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":1,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":121,"dataRead":58,"rowCount":6,"jobId":134,"name":"save at <unknown>:0","description":"Job group for statement 21:\n#Thanks to MIM for help with this code\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/DimScenario.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/DimScenario\")","submissionTime":"2023-07-01T17:48:17.241GMT","completionTime":"2023-07-01T17:48:17.281GMT","stageIds":[197],"jobGroup":"21","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":58,"rowCount":1,"jobId":133,"name":"load at <unknown>:0","description":"Job group for statement 21:\n#Thanks to MIM for help with this code\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/DimScenario.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/DimScenario\")","submissionTime":"2023-07-01T17:48:17.044GMT","completionTime":"2023-07-01T17:48:17.080GMT","stageIds":[196],"jobGroup":"21","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}}],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"2ddf20de-5707-4175-a81a-b215773d44ac"},"text/plain":"StatementMeta(, 3c166b9a-f167-4f97-8747-8e1fdca2eefa, 21, Finished, Available)"},"metadata":{}}],"execution_count":19,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}}},{"cell_type":"code","source":["#Thanks to MIM for help with this code\r\n","spark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\r\n","spark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\r\n","#Load from the default lakehouse, make sure you click on the pin <=============\r\n","from pyspark.sql.types import *\r\n","df = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/FactAdditionalInternationalProductDescription.csv\")\r\n","df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/FactAdditionalInternationalProductDescription\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"3c166b9a-f167-4f97-8747-8e1fdca2eefa","statement_id":22,"state":"finished","livy_statement_state":"available","queued_time":"2023-07-01T17:47:29.2675719Z","session_start_time":null,"execution_start_time":"2023-07-01T17:48:19.9784025Z","execution_finish_time":"2023-07-01T17:48:22.5771973Z","spark_jobs":{"numbers":{"RUNNING":0,"FAILED":0,"SUCCEEDED":7,"UNKNOWN":0},"jobs":[{"dataWritten":0,"dataRead":4341,"rowCount":50,"jobId":146,"name":"toString at String.java:2994","description":"Delta: Job group for statement 22:\n#Thanks to MIM for help with this code\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/FactAdditionalInternationalProductDescription.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/FactAdditionalInternationalProductDescription\"): Compute snapshot for version: 0","submissionTime":"2023-07-01T17:48:21.786GMT","completionTime":"2023-07-01T17:48:21.807GMT","stageIds":[215,213,214],"jobGroup":"22","status":"SUCCEEDED","numTasks":52,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":51,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":2,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":4341,"dataRead":1641,"rowCount":54,"jobId":145,"name":"toString at String.java:2994","description":"Delta: Job group for statement 22:\n#Thanks to MIM for help with this code\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/FactAdditionalInternationalProductDescription.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/FactAdditionalInternationalProductDescription\"): Compute snapshot for version: 0","submissionTime":"2023-07-01T17:48:21.454GMT","completionTime":"2023-07-01T17:48:21.775GMT","stageIds":[212,211],"jobGroup":"22","status":"SUCCEEDED","numTasks":51,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":1,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":1641,"dataRead":1535,"rowCount":8,"jobId":144,"name":"toString at String.java:2994","description":"Delta: Job group for statement 22:\n#Thanks to MIM for help with this code\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/FactAdditionalInternationalProductDescription.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/FactAdditionalInternationalProductDescription\"): Compute snapshot for version: 0","submissionTime":"2023-07-01T17:48:21.312GMT","completionTime":"2023-07-01T17:48:21.353GMT","stageIds":[210],"jobGroup":"22","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":0,"rowCount":0,"jobId":143,"name":"","description":"Job group for statement 22:\n#Thanks to MIM for help with this code\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/FactAdditionalInternationalProductDescription.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/FactAdditionalInternationalProductDescription\")","submissionTime":"2023-07-01T17:48:20.800GMT","completionTime":"2023-07-01T17:48:20.800GMT","stageIds":[],"jobGroup":"22","status":"SUCCEEDED","numTasks":0,"numActiveTasks":0,"numCompletedTasks":0,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":0,"numActiveStages":0,"numCompletedStages":0,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":267432,"dataRead":562930,"rowCount":30336,"jobId":142,"name":"save at <unknown>:0","description":"Job group for statement 22:\n#Thanks to MIM for help with this code\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/FactAdditionalInternationalProductDescription.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/FactAdditionalInternationalProductDescription\")","submissionTime":"2023-07-01T17:48:20.441GMT","completionTime":"2023-07-01T17:48:20.715GMT","stageIds":[208,209],"jobGroup":"22","status":"SUCCEEDED","numTasks":2,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":1,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":562930,"dataRead":1927911,"rowCount":30336,"jobId":141,"name":"save at <unknown>:0","description":"Job group for statement 22:\n#Thanks to MIM for help with this code\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/FactAdditionalInternationalProductDescription.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/FactAdditionalInternationalProductDescription\")","submissionTime":"2023-07-01T17:48:20.289GMT","completionTime":"2023-07-01T17:48:20.394GMT","stageIds":[207],"jobGroup":"22","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":65536,"rowCount":1,"jobId":140,"name":"load at <unknown>:0","description":"Job group for statement 22:\n#Thanks to MIM for help with this code\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/FactAdditionalInternationalProductDescription.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/FactAdditionalInternationalProductDescription\")","submissionTime":"2023-07-01T17:48:20.038GMT","completionTime":"2023-07-01T17:48:20.102GMT","stageIds":[206],"jobGroup":"22","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}}],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"9e47cde9-2ed5-4c16-be25-1bee165964f8"},"text/plain":"StatementMeta(, 3c166b9a-f167-4f97-8747-8e1fdca2eefa, 22, Finished, Available)"},"metadata":{}}],"execution_count":20,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"advisor":{"adviceMetadata":"{\"artifactId\":\"4b6dc142-53a6-4302-92d0-b0982019ff67\",\"activityId\":\"3c166b9a-f167-4f97-8747-8e1fdca2eefa\",\"applicationId\":\"application_1688218711538_0001\",\"jobGroupId\":\"22\",\"advices\":{\"info\":1}}"}}},{"cell_type":"code","source":["#Thanks to MIM for help with this code\r\n","spark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\r\n","spark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\r\n","#Load from the default lakehouse, make sure you click on the pin <=============\r\n","from pyspark.sql.types import *\r\n","df = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/FactCallCenter.csv\")\r\n","df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/FactCallCenter\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"3c166b9a-f167-4f97-8747-8e1fdca2eefa","statement_id":23,"state":"finished","livy_statement_state":"available","queued_time":"2023-07-01T17:47:29.3637504Z","session_start_time":null,"execution_start_time":"2023-07-01T17:48:22.9418685Z","execution_finish_time":"2023-07-01T17:48:25.6159926Z","spark_jobs":{"numbers":{"RUNNING":0,"FAILED":0,"SUCCEEDED":7,"UNKNOWN":0},"jobs":[{"dataWritten":0,"dataRead":4487,"rowCount":50,"jobId":153,"name":"toString at String.java:2994","description":"Delta: Job group for statement 23:\n#Thanks to MIM for help with this code\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/FactCallCenter.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/FactCallCenter\"): Compute snapshot for version: 0","submissionTime":"2023-07-01T17:48:24.563GMT","completionTime":"2023-07-01T17:48:24.588GMT","stageIds":[223,224,225],"jobGroup":"23","status":"SUCCEEDED","numTasks":52,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":51,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":2,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":4487,"dataRead":1988,"rowCount":54,"jobId":152,"name":"toString at String.java:2994","description":"Delta: Job group for statement 23:\n#Thanks to MIM for help with this code\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/FactCallCenter.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/FactCallCenter\"): Compute snapshot for version: 0","submissionTime":"2023-07-01T17:48:24.170GMT","completionTime":"2023-07-01T17:48:24.547GMT","stageIds":[222,221],"jobGroup":"23","status":"SUCCEEDED","numTasks":51,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":1,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":1988,"dataRead":3114,"rowCount":8,"jobId":151,"name":"toString at String.java:2994","description":"Delta: Job group for statement 23:\n#Thanks to MIM for help with this code\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/FactCallCenter.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/FactCallCenter\"): Compute snapshot for version: 0","submissionTime":"2023-07-01T17:48:24.014GMT","completionTime":"2023-07-01T17:48:24.056GMT","stageIds":[220],"jobGroup":"23","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":0,"rowCount":0,"jobId":150,"name":"","description":"Job group for statement 23:\n#Thanks to MIM for help with this code\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/FactCallCenter.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/FactCallCenter\")","submissionTime":"2023-07-01T17:48:23.655GMT","completionTime":"2023-07-01T17:48:23.655GMT","stageIds":[],"jobGroup":"23","status":"SUCCEEDED","numTasks":0,"numActiveTasks":0,"numCompletedTasks":0,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":0,"numActiveStages":0,"numCompletedStages":0,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":10517,"dataRead":7095,"rowCount":240,"jobId":149,"name":"save at <unknown>:0","description":"Job group for statement 23:\n#Thanks to MIM for help with this code\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/FactCallCenter.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/FactCallCenter\")","submissionTime":"2023-07-01T17:48:23.338GMT","completionTime":"2023-07-01T17:48:23.557GMT","stageIds":[219,218],"jobGroup":"23","status":"SUCCEEDED","numTasks":2,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":1,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":7095,"dataRead":11148,"rowCount":240,"jobId":148,"name":"save at <unknown>:0","description":"Job group for statement 23:\n#Thanks to MIM for help with this code\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/FactCallCenter.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/FactCallCenter\")","submissionTime":"2023-07-01T17:48:23.265GMT","completionTime":"2023-07-01T17:48:23.304GMT","stageIds":[217],"jobGroup":"23","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":11148,"rowCount":1,"jobId":147,"name":"load at <unknown>:0","description":"Job group for statement 23:\n#Thanks to MIM for help with this code\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/FactCallCenter.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/FactCallCenter\")","submissionTime":"2023-07-01T17:48:23.053GMT","completionTime":"2023-07-01T17:48:23.084GMT","stageIds":[216],"jobGroup":"23","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}}],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"fa05143d-af0e-4552-9249-6f3af2295603"},"text/plain":"StatementMeta(, 3c166b9a-f167-4f97-8747-8e1fdca2eefa, 23, Finished, Available)"},"metadata":{}}],"execution_count":21,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}}},{"cell_type":"code","source":["#Thanks to MIM for help with this code\r\n","spark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\r\n","spark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\r\n","#Load from the default lakehouse, make sure you click on the pin <=============\r\n","from pyspark.sql.types import *\r\n","df = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/FactCurrencyRate.csv\")\r\n","df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/FactCurrencyRate\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"3c166b9a-f167-4f97-8747-8e1fdca2eefa","statement_id":24,"state":"finished","livy_statement_state":"available","queued_time":"2023-07-01T17:47:29.455761Z","session_start_time":null,"execution_start_time":"2023-07-01T17:48:25.9685647Z","execution_finish_time":"2023-07-01T17:48:28.6376613Z","spark_jobs":{"numbers":{"RUNNING":0,"FAILED":0,"SUCCEEDED":7,"UNKNOWN":0},"jobs":[{"dataWritten":0,"dataRead":4351,"rowCount":50,"jobId":160,"name":"toString at String.java:2994","description":"Delta: Job group for statement 24:\n#Thanks to MIM for help with this code\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/FactCurrencyRate.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/FactCurrencyRate\"): Compute snapshot for version: 0","submissionTime":"2023-07-01T17:48:27.624GMT","completionTime":"2023-07-01T17:48:27.645GMT","stageIds":[233,234,235],"jobGroup":"24","status":"SUCCEEDED","numTasks":52,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":51,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":2,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":4351,"dataRead":1671,"rowCount":54,"jobId":159,"name":"toString at String.java:2994","description":"Delta: Job group for statement 24:\n#Thanks to MIM for help with this code\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/FactCurrencyRate.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/FactCurrencyRate\"): Compute snapshot for version: 0","submissionTime":"2023-07-01T17:48:27.291GMT","completionTime":"2023-07-01T17:48:27.611GMT","stageIds":[231,232],"jobGroup":"24","status":"SUCCEEDED","numTasks":51,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":1,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":1671,"dataRead":1776,"rowCount":8,"jobId":158,"name":"toString at String.java:2994","description":"Delta: Job group for statement 24:\n#Thanks to MIM for help with this code\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/FactCurrencyRate.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/FactCurrencyRate\"): Compute snapshot for version: 0","submissionTime":"2023-07-01T17:48:27.143GMT","completionTime":"2023-07-01T17:48:27.191GMT","stageIds":[230],"jobGroup":"24","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":0,"rowCount":0,"jobId":157,"name":"","description":"Job group for statement 24:\n#Thanks to MIM for help with this code\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/FactCurrencyRate.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/FactCurrencyRate\")","submissionTime":"2023-07-01T17:48:26.767GMT","completionTime":"2023-07-01T17:48:26.767GMT","stageIds":[],"jobGroup":"24","status":"SUCCEEDED","numTasks":0,"numActiveTasks":0,"numCompletedTasks":0,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":0,"numActiveStages":0,"numCompletedStages":0,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":280599,"dataRead":432256,"rowCount":28528,"jobId":156,"name":"save at <unknown>:0","description":"Job group for statement 24:\n#Thanks to MIM for help with this code\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/FactCurrencyRate.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/FactCurrencyRate\")","submissionTime":"2023-07-01T17:48:26.429GMT","completionTime":"2023-07-01T17:48:26.667GMT","stageIds":[228,229],"jobGroup":"24","status":"SUCCEEDED","numTasks":2,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":1,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":432256,"dataRead":1049978,"rowCount":28528,"jobId":155,"name":"save at <unknown>:0","description":"Job group for statement 24:\n#Thanks to MIM for help with this code\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/FactCurrencyRate.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/FactCurrencyRate\")","submissionTime":"2023-07-01T17:48:26.319GMT","completionTime":"2023-07-01T17:48:26.395GMT","stageIds":[227],"jobGroup":"24","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":65536,"rowCount":1,"jobId":154,"name":"load at <unknown>:0","description":"Job group for statement 24:\n#Thanks to MIM for help with this code\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/FactCurrencyRate.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/FactCurrencyRate\")","submissionTime":"2023-07-01T17:48:26.027GMT","completionTime":"2023-07-01T17:48:26.089GMT","stageIds":[226],"jobGroup":"24","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}}],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"ac295111-c93a-41a1-87ad-2d073299c9b0"},"text/plain":"StatementMeta(, 3c166b9a-f167-4f97-8747-8e1fdca2eefa, 24, Finished, Available)"},"metadata":{}}],"execution_count":22,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}}},{"cell_type":"code","source":["#Thanks to MIM for help with this code\r\n","spark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\r\n","spark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\r\n","#Load from the default lakehouse, make sure you click on the pin\r\n","from pyspark.sql.types import *\r\n","df = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/FactFinance.csv\")\r\n","df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/FactFinance\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"3c166b9a-f167-4f97-8747-8e1fdca2eefa","statement_id":25,"state":"finished","livy_statement_state":"available","queued_time":"2023-07-01T17:47:29.5598944Z","session_start_time":null,"execution_start_time":"2023-07-01T17:48:28.9826548Z","execution_finish_time":"2023-07-01T17:48:31.6028631Z","spark_jobs":{"numbers":{"RUNNING":0,"FAILED":0,"SUCCEEDED":7,"UNKNOWN":0},"jobs":[{"dataWritten":0,"dataRead":4384,"rowCount":50,"jobId":167,"name":"toString at String.java:2994","description":"Delta: Job group for statement 25:\n#Thanks to MIM for help with this code\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/FactFinance.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/FactFinance\"): Compute snapshot for version: 0","submissionTime":"2023-07-01T17:48:30.815GMT","completionTime":"2023-07-01T17:48:30.837GMT","stageIds":[245,243,244],"jobGroup":"25","status":"SUCCEEDED","numTasks":52,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":51,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":2,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":4384,"dataRead":1719,"rowCount":54,"jobId":166,"name":"toString at String.java:2994","description":"Delta: Job group for statement 25:\n#Thanks to MIM for help with this code\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/FactFinance.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/FactFinance\"): Compute snapshot for version: 0","submissionTime":"2023-07-01T17:48:30.511GMT","completionTime":"2023-07-01T17:48:30.799GMT","stageIds":[241,242],"jobGroup":"25","status":"SUCCEEDED","numTasks":51,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":1,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":1719,"dataRead":2155,"rowCount":8,"jobId":165,"name":"toString at String.java:2994","description":"Delta: Job group for statement 25:\n#Thanks to MIM for help with this code\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/FactFinance.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/FactFinance\"): Compute snapshot for version: 0","submissionTime":"2023-07-01T17:48:30.374GMT","completionTime":"2023-07-01T17:48:30.412GMT","stageIds":[240],"jobGroup":"25","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":0,"rowCount":0,"jobId":164,"name":"","description":"Job group for statement 25:\n#Thanks to MIM for help with this code\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/FactFinance.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/FactFinance\")","submissionTime":"2023-07-01T17:48:29.976GMT","completionTime":"2023-07-01T17:48:29.976GMT","stageIds":[],"jobGroup":"25","status":"SUCCEEDED","numTasks":0,"numActiveTasks":0,"numCompletedTasks":0,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":0,"numActiveStages":0,"numCompletedStages":0,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":438299,"dataRead":880396,"rowCount":78818,"jobId":163,"name":"save at <unknown>:0","description":"Job group for statement 25:\n#Thanks to MIM for help with this code\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/FactFinance.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/FactFinance\")","submissionTime":"2023-07-01T17:48:29.516GMT","completionTime":"2023-07-01T17:48:29.874GMT","stageIds":[238,239],"jobGroup":"25","status":"SUCCEEDED","numTasks":2,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":1,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":880396,"dataRead":2305234,"rowCount":78818,"jobId":162,"name":"save at <unknown>:0","description":"Job group for statement 25:\n#Thanks to MIM for help with this code\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/FactFinance.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/FactFinance\")","submissionTime":"2023-07-01T17:48:29.275GMT","completionTime":"2023-07-01T17:48:29.482GMT","stageIds":[237],"jobGroup":"25","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":65536,"rowCount":1,"jobId":161,"name":"load at <unknown>:0","description":"Job group for statement 25:\n#Thanks to MIM for help with this code\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/FactFinance.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/FactFinance\")","submissionTime":"2023-07-01T17:48:29.035GMT","completionTime":"2023-07-01T17:48:29.106GMT","stageIds":[236],"jobGroup":"25","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}}],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"1dc3d957-1503-4561-ac1b-9520d60ea442"},"text/plain":"StatementMeta(, 3c166b9a-f167-4f97-8747-8e1fdca2eefa, 25, Finished, Available)"},"metadata":{}}],"execution_count":23,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}}},{"cell_type":"code","source":["#Thanks to MIM for help with this code\r\n","spark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\r\n","spark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\r\n","#Load from the default lakehouse, make sure you click on the pin <=============\r\n","from pyspark.sql.types import *\r\n","df = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/FactInternetSales.csv\")\r\n","df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/FactInternetSales\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"3c166b9a-f167-4f97-8747-8e1fdca2eefa","statement_id":26,"state":"finished","livy_statement_state":"available","queued_time":"2023-07-01T17:47:29.6562068Z","session_start_time":null,"execution_start_time":"2023-07-01T17:48:31.9794207Z","execution_finish_time":"2023-07-01T17:48:35.8672843Z","spark_jobs":{"numbers":{"RUNNING":0,"FAILED":0,"SUCCEEDED":7,"UNKNOWN":0},"jobs":[{"dataWritten":0,"dataRead":4608,"rowCount":50,"jobId":174,"name":"toString at String.java:2994","description":"Delta: Job group for statement 26:\n#Thanks to MIM for help with this code\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/FactInternetSales.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/FactInternetSales\"): Compute snapshot for version: 0","submissionTime":"2023-07-01T17:48:34.488GMT","completionTime":"2023-07-01T17:48:34.511GMT","stageIds":[255,253,254],"jobGroup":"26","status":"SUCCEEDED","numTasks":52,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":51,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":2,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":4608,"dataRead":2490,"rowCount":54,"jobId":173,"name":"toString at String.java:2994","description":"Delta: Job group for statement 26:\n#Thanks to MIM for help with this code\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/FactInternetSales.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/FactInternetSales\"): Compute snapshot for version: 0","submissionTime":"2023-07-01T17:48:34.156GMT","completionTime":"2023-07-01T17:48:34.471GMT","stageIds":[251,252],"jobGroup":"26","status":"SUCCEEDED","numTasks":51,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":1,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":2490,"dataRead":5174,"rowCount":8,"jobId":172,"name":"toString at String.java:2994","description":"Delta: Job group for statement 26:\n#Thanks to MIM for help with this code\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/FactInternetSales.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/FactInternetSales\"): Compute snapshot for version: 0","submissionTime":"2023-07-01T17:48:34.010GMT","completionTime":"2023-07-01T17:48:34.052GMT","stageIds":[250],"jobGroup":"26","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":0,"rowCount":0,"jobId":171,"name":"","description":"Job group for statement 26:\n#Thanks to MIM for help with this code\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/FactInternetSales.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/FactInternetSales\")","submissionTime":"2023-07-01T17:48:33.619GMT","completionTime":"2023-07-01T17:48:33.619GMT","stageIds":[],"jobGroup":"26","status":"SUCCEEDED","numTasks":0,"numActiveTasks":0,"numCompletedTasks":0,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":0,"numActiveStages":0,"numCompletedStages":0,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":1068305,"dataRead":2699801,"rowCount":120796,"jobId":170,"name":"save at <unknown>:0","description":"Job group for statement 26:\n#Thanks to MIM for help with this code\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/FactInternetSales.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/FactInternetSales\")","submissionTime":"2023-07-01T17:48:32.698GMT","completionTime":"2023-07-01T17:48:33.482GMT","stageIds":[248,249],"jobGroup":"26","status":"SUCCEEDED","numTasks":4,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":3,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":2699801,"dataRead":12401895,"rowCount":120796,"jobId":169,"name":"save at <unknown>:0","description":"Job group for statement 26:\n#Thanks to MIM for help with this code\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/FactInternetSales.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/FactInternetSales\")","submissionTime":"2023-07-01T17:48:32.309GMT","completionTime":"2023-07-01T17:48:32.664GMT","stageIds":[247],"jobGroup":"26","status":"SUCCEEDED","numTasks":3,"numActiveTasks":0,"numCompletedTasks":3,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":3,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":65536,"rowCount":1,"jobId":168,"name":"load at <unknown>:0","description":"Job group for statement 26:\n#Thanks to MIM for help with this code\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/FactInternetSales.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/FactInternetSales\")","submissionTime":"2023-07-01T17:48:32.016GMT","completionTime":"2023-07-01T17:48:32.116GMT","stageIds":[246],"jobGroup":"26","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}}],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"01138ce4-e14e-4142-9e88-2ee706c2fb2c"},"text/plain":"StatementMeta(, 3c166b9a-f167-4f97-8747-8e1fdca2eefa, 26, Finished, Available)"},"metadata":{}}],"execution_count":24,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}}},{"cell_type":"code","source":["#Thanks to MIM for help with this code\r\n","spark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\r\n","spark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\r\n","#Load from the default lakehouse, make sure you click on the pin\r\n","from pyspark.sql.types import *\r\n","df = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/FactInternetSalesReason.csv\")\r\n","df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/FactInternetSalesReason\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"3c166b9a-f167-4f97-8747-8e1fdca2eefa","statement_id":27,"state":"finished","livy_statement_state":"available","queued_time":"2023-07-01T17:47:29.7611245Z","session_start_time":null,"execution_start_time":"2023-07-01T17:48:36.2165881Z","execution_finish_time":"2023-07-01T17:48:38.8774338Z","spark_jobs":{"numbers":{"RUNNING":0,"FAILED":0,"SUCCEEDED":7,"UNKNOWN":0},"jobs":[{"dataWritten":0,"dataRead":4337,"rowCount":50,"jobId":181,"name":"toString at String.java:2994","description":"Delta: Job group for statement 27:\n#Thanks to MIM for help with this code\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/FactInternetSalesReason.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/FactInternetSalesReason\"): Compute snapshot for version: 0","submissionTime":"2023-07-01T17:48:37.912GMT","completionTime":"2023-07-01T17:48:37.933GMT","stageIds":[263,264,265],"jobGroup":"27","status":"SUCCEEDED","numTasks":52,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":51,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":2,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":4337,"dataRead":1539,"rowCount":54,"jobId":180,"name":"toString at String.java:2994","description":"Delta: Job group for statement 27:\n#Thanks to MIM for help with this code\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/FactInternetSalesReason.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/FactInternetSalesReason\"): Compute snapshot for version: 0","submissionTime":"2023-07-01T17:48:37.554GMT","completionTime":"2023-07-01T17:48:37.899GMT","stageIds":[261,262],"jobGroup":"27","status":"SUCCEEDED","numTasks":51,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":1,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":1539,"dataRead":1473,"rowCount":8,"jobId":179,"name":"toString at String.java:2994","description":"Delta: Job group for statement 27:\n#Thanks to MIM for help with this code\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/FactInternetSalesReason.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/FactInternetSalesReason\"): Compute snapshot for version: 0","submissionTime":"2023-07-01T17:48:37.415GMT","completionTime":"2023-07-01T17:48:37.455GMT","stageIds":[260],"jobGroup":"27","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":0,"rowCount":0,"jobId":178,"name":"","description":"Job group for statement 27:\n#Thanks to MIM for help with this code\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/FactInternetSalesReason.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/FactInternetSalesReason\")","submissionTime":"2023-07-01T17:48:36.989GMT","completionTime":"2023-07-01T17:48:36.989GMT","stageIds":[],"jobGroup":"27","status":"SUCCEEDED","numTasks":0,"numActiveTasks":0,"numCompletedTasks":0,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":0,"numActiveStages":0,"numCompletedStages":0,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":224323,"dataRead":418978,"rowCount":129030,"jobId":177,"name":"save at <unknown>:0","description":"Job group for statement 27:\n#Thanks to MIM for help with this code\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/FactInternetSalesReason.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/FactInternetSalesReason\")","submissionTime":"2023-07-01T17:48:36.662GMT","completionTime":"2023-07-01T17:48:36.914GMT","stageIds":[259,258],"jobGroup":"27","status":"SUCCEEDED","numTasks":2,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":1,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":418978,"dataRead":842402,"rowCount":129030,"jobId":176,"name":"save at <unknown>:0","description":"Job group for statement 27:\n#Thanks to MIM for help with this code\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/FactInternetSalesReason.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/FactInternetSalesReason\")","submissionTime":"2023-07-01T17:48:36.508GMT","completionTime":"2023-07-01T17:48:36.627GMT","stageIds":[257],"jobGroup":"27","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":65536,"rowCount":1,"jobId":175,"name":"load at <unknown>:0","description":"Job group for statement 27:\n#Thanks to MIM for help with this code\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/FactInternetSalesReason.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/FactInternetSalesReason\")","submissionTime":"2023-07-01T17:48:36.276GMT","completionTime":"2023-07-01T17:48:36.336GMT","stageIds":[256],"jobGroup":"27","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}}],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"0f72b505-71a4-4c5f-8907-7585a2b58dc5"},"text/plain":"StatementMeta(, 3c166b9a-f167-4f97-8747-8e1fdca2eefa, 27, Finished, Available)"},"metadata":{}}],"execution_count":25,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}}},{"cell_type":"code","source":["#Thanks to MIM for help with this code\r\n","spark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\r\n","spark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\r\n","#Load from the default lakehouse, make sure you click on the pin <=============\r\n","from pyspark.sql.types import *\r\n","df = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/FactProductInventory.csv\")\r\n","df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/FactProductInventory\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"3c166b9a-f167-4f97-8747-8e1fdca2eefa","statement_id":28,"state":"finished","livy_statement_state":"available","queued_time":"2023-07-01T17:47:29.9076112Z","session_start_time":null,"execution_start_time":"2023-07-01T17:48:39.2103847Z","execution_finish_time":"2023-07-01T17:48:44.3696342Z","spark_jobs":{"numbers":{"RUNNING":0,"FAILED":0,"SUCCEEDED":7,"UNKNOWN":0},"jobs":[{"dataWritten":0,"dataRead":4376,"rowCount":50,"jobId":188,"name":"toString at String.java:2994","description":"Delta: Job group for statement 28:\n#Thanks to MIM for help with this code\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/FactProductInventory.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/FactProductInventory\"): Compute snapshot for version: 0","submissionTime":"2023-07-01T17:48:42.830GMT","completionTime":"2023-07-01T17:48:42.850GMT","stageIds":[273,274,275],"jobGroup":"28","status":"SUCCEEDED","numTasks":52,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":51,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":2,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":4376,"dataRead":1681,"rowCount":54,"jobId":187,"name":"toString at String.java:2994","description":"Delta: Job group for statement 28:\n#Thanks to MIM for help with this code\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/FactProductInventory.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/FactProductInventory\"): Compute snapshot for version: 0","submissionTime":"2023-07-01T17:48:42.542GMT","completionTime":"2023-07-01T17:48:42.817GMT","stageIds":[271,272],"jobGroup":"28","status":"SUCCEEDED","numTasks":51,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":1,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":1681,"dataRead":1965,"rowCount":8,"jobId":186,"name":"toString at String.java:2994","description":"Delta: Job group for statement 28:\n#Thanks to MIM for help with this code\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/FactProductInventory.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/FactProductInventory\"): Compute snapshot for version: 0","submissionTime":"2023-07-01T17:48:42.386GMT","completionTime":"2023-07-01T17:48:42.426GMT","stageIds":[270],"jobGroup":"28","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":0,"rowCount":0,"jobId":185,"name":"","description":"Job group for statement 28:\n#Thanks to MIM for help with this code\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/FactProductInventory.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/FactProductInventory\")","submissionTime":"2023-07-01T17:48:41.981GMT","completionTime":"2023-07-01T17:48:41.981GMT","stageIds":[],"jobGroup":"28","status":"SUCCEEDED","numTasks":0,"numActiveTasks":0,"numCompletedTasks":0,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":0,"numActiveStages":0,"numCompletedStages":0,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":2991229,"dataRead":10367345,"rowCount":1552572,"jobId":184,"name":"save at <unknown>:0","description":"Job group for statement 28:\n#Thanks to MIM for help with this code\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/FactProductInventory.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/FactProductInventory\")","submissionTime":"2023-07-01T17:48:40.393GMT","completionTime":"2023-07-01T17:48:41.910GMT","stageIds":[269,268],"jobGroup":"28","status":"SUCCEEDED","numTasks":9,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":8,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":10367345,"dataRead":31535642,"rowCount":1552572,"jobId":183,"name":"save at <unknown>:0","description":"Job group for statement 28:\n#Thanks to MIM for help with this code\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/FactProductInventory.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/FactProductInventory\")","submissionTime":"2023-07-01T17:48:39.557GMT","completionTime":"2023-07-01T17:48:40.354GMT","stageIds":[267],"jobGroup":"28","status":"SUCCEEDED","numTasks":8,"numActiveTasks":0,"numCompletedTasks":8,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":8,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":65536,"rowCount":1,"jobId":182,"name":"load at <unknown>:0","description":"Job group for statement 28:\n#Thanks to MIM for help with this code\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/FactProductInventory.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/FactProductInventory\")","submissionTime":"2023-07-01T17:48:39.271GMT","completionTime":"2023-07-01T17:48:39.382GMT","stageIds":[266],"jobGroup":"28","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}}],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"81b7f184-60b9-4181-867f-0c8b809c86e3"},"text/plain":"StatementMeta(, 3c166b9a-f167-4f97-8747-8e1fdca2eefa, 28, Finished, Available)"},"metadata":{}}],"execution_count":26,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}}},{"cell_type":"code","source":["#Thanks to MIM for help with this code\r\n","spark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\r\n","spark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\r\n","#Load from the default lakehouse, make sure you click on the pin <=============\r\n","from pyspark.sql.types import *\r\n","df = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/FactResellerSales.csv\")\r\n","df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/FactResellerSales\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"3c166b9a-f167-4f97-8747-8e1fdca2eefa","statement_id":29,"state":"finished","livy_statement_state":"available","queued_time":"2023-07-01T17:47:30.0206305Z","session_start_time":null,"execution_start_time":"2023-07-01T17:48:44.7093961Z","execution_finish_time":"2023-07-01T17:48:48.4895261Z","spark_jobs":{"numbers":{"RUNNING":0,"FAILED":0,"SUCCEEDED":7,"UNKNOWN":0},"jobs":[{"dataWritten":0,"dataRead":4631,"rowCount":50,"jobId":195,"name":"toString at String.java:2994","description":"Delta: Job group for statement 29:\n#Thanks to MIM for help with this code\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/FactResellerSales.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/FactResellerSales\"): Compute snapshot for version: 0","submissionTime":"2023-07-01T17:48:47.294GMT","completionTime":"2023-07-01T17:48:47.315GMT","stageIds":[284,285,283],"jobGroup":"29","status":"SUCCEEDED","numTasks":52,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":51,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":2,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":4631,"dataRead":2683,"rowCount":54,"jobId":194,"name":"toString at String.java:2994","description":"Delta: Job group for statement 29:\n#Thanks to MIM for help with this code\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/FactResellerSales.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/FactResellerSales\"): Compute snapshot for version: 0","submissionTime":"2023-07-01T17:48:46.975GMT","completionTime":"2023-07-01T17:48:47.274GMT","stageIds":[281,282],"jobGroup":"29","status":"SUCCEEDED","numTasks":51,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":1,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":2683,"dataRead":5383,"rowCount":8,"jobId":193,"name":"toString at String.java:2994","description":"Delta: Job group for statement 29:\n#Thanks to MIM for help with this code\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/FactResellerSales.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/FactResellerSales\"): Compute snapshot for version: 0","submissionTime":"2023-07-01T17:48:46.794GMT","completionTime":"2023-07-01T17:48:46.841GMT","stageIds":[280],"jobGroup":"29","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":0,"rowCount":0,"jobId":192,"name":"","description":"Job group for statement 29:\n#Thanks to MIM for help with this code\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/FactResellerSales.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/FactResellerSales\")","submissionTime":"2023-07-01T17:48:46.374GMT","completionTime":"2023-07-01T17:48:46.374GMT","stageIds":[],"jobGroup":"29","status":"SUCCEEDED","numTasks":0,"numActiveTasks":0,"numCompletedTasks":0,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":0,"numActiveStages":0,"numCompletedStages":0,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":1217252,"dataRead":4224283,"rowCount":121710,"jobId":191,"name":"save at <unknown>:0","description":"Job group for statement 29:\n#Thanks to MIM for help with this code\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/FactResellerSales.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/FactResellerSales\")","submissionTime":"2023-07-01T17:48:45.489GMT","completionTime":"2023-07-01T17:48:46.300GMT","stageIds":[278,279],"jobGroup":"29","status":"SUCCEEDED","numTasks":5,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":4,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":4224283,"dataRead":14210885,"rowCount":121710,"jobId":190,"name":"save at <unknown>:0","description":"Job group for statement 29:\n#Thanks to MIM for help with this code\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/FactResellerSales.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/FactResellerSales\")","submissionTime":"2023-07-01T17:48:45.131GMT","completionTime":"2023-07-01T17:48:45.453GMT","stageIds":[277],"jobGroup":"29","status":"SUCCEEDED","numTasks":4,"numActiveTasks":0,"numCompletedTasks":4,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":4,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":65536,"rowCount":1,"jobId":189,"name":"load at <unknown>:0","description":"Job group for statement 29:\n#Thanks to MIM for help with this code\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/FactResellerSales.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/FactResellerSales\")","submissionTime":"2023-07-01T17:48:44.781GMT","completionTime":"2023-07-01T17:48:44.938GMT","stageIds":[276],"jobGroup":"29","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}}],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"3163ef89-c8c6-4a01-84de-c24e73f8040c"},"text/plain":"StatementMeta(, 3c166b9a-f167-4f97-8747-8e1fdca2eefa, 29, Finished, Available)"},"metadata":{}}],"execution_count":27,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}}},{"cell_type":"code","source":["#Thanks to MIM for help with this code\r\n","spark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\r\n","spark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\r\n","#Load from the default lakehouse, make sure you click on the pin <=============\r\n","from pyspark.sql.types import *\r\n","df = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/FactSalesQuota.csv\")\r\n","df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/FactSalesQuota\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"3c166b9a-f167-4f97-8747-8e1fdca2eefa","statement_id":30,"state":"finished","livy_statement_state":"available","queued_time":"2023-07-01T17:47:30.1402111Z","session_start_time":null,"execution_start_time":"2023-07-01T17:48:48.8343849Z","execution_finish_time":"2023-07-01T17:48:51.4689533Z","spark_jobs":{"numbers":{"RUNNING":0,"FAILED":0,"SUCCEEDED":7,"UNKNOWN":0},"jobs":[{"dataWritten":0,"dataRead":4384,"rowCount":50,"jobId":202,"name":"toString at String.java:2994","description":"Delta: Job group for statement 30:\n#Thanks to MIM for help with this code\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/FactSalesQuota.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/FactSalesQuota\"): Compute snapshot for version: 0","submissionTime":"2023-07-01T17:48:50.338GMT","completionTime":"2023-07-01T17:48:50.359GMT","stageIds":[293,294,295],"jobGroup":"30","status":"SUCCEEDED","numTasks":52,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":51,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":2,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":4384,"dataRead":1676,"rowCount":54,"jobId":201,"name":"toString at String.java:2994","description":"Delta: Job group for statement 30:\n#Thanks to MIM for help with this code\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/FactSalesQuota.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/FactSalesQuota\"): Compute snapshot for version: 0","submissionTime":"2023-07-01T17:48:49.953GMT","completionTime":"2023-07-01T17:48:50.326GMT","stageIds":[291,292],"jobGroup":"30","status":"SUCCEEDED","numTasks":51,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":1,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":1676,"dataRead":2048,"rowCount":8,"jobId":200,"name":"toString at String.java:2994","description":"Delta: Job group for statement 30:\n#Thanks to MIM for help with this code\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/FactSalesQuota.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/FactSalesQuota\"): Compute snapshot for version: 0","submissionTime":"2023-07-01T17:48:49.829GMT","completionTime":"2023-07-01T17:48:49.867GMT","stageIds":[290],"jobGroup":"30","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":0,"rowCount":0,"jobId":199,"name":"","description":"Job group for statement 30:\n#Thanks to MIM for help with this code\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/FactSalesQuota.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/FactSalesQuota\")","submissionTime":"2023-07-01T17:48:49.443GMT","completionTime":"2023-07-01T17:48:49.443GMT","stageIds":[],"jobGroup":"30","status":"SUCCEEDED","numTasks":0,"numActiveTasks":0,"numCompletedTasks":0,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":0,"numActiveStages":0,"numCompletedStages":0,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":5959,"dataRead":3735,"rowCount":326,"jobId":198,"name":"save at <unknown>:0","description":"Job group for statement 30:\n#Thanks to MIM for help with this code\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/FactSalesQuota.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/FactSalesQuota\")","submissionTime":"2023-07-01T17:48:49.175GMT","completionTime":"2023-07-01T17:48:49.369GMT","stageIds":[288,289],"jobGroup":"30","status":"SUCCEEDED","numTasks":2,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":1,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":3735,"dataRead":9930,"rowCount":326,"jobId":197,"name":"save at <unknown>:0","description":"Job group for statement 30:\n#Thanks to MIM for help with this code\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/FactSalesQuota.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/FactSalesQuota\")","submissionTime":"2023-07-01T17:48:49.106GMT","completionTime":"2023-07-01T17:48:49.137GMT","stageIds":[287],"jobGroup":"30","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":9930,"rowCount":1,"jobId":196,"name":"load at <unknown>:0","description":"Job group for statement 30:\n#Thanks to MIM for help with this code\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/FactSalesQuota.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/FactSalesQuota\")","submissionTime":"2023-07-01T17:48:48.896GMT","completionTime":"2023-07-01T17:48:48.927GMT","stageIds":[286],"jobGroup":"30","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}}],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"3763107a-172e-4ae4-8d6b-070e98e9254b"},"text/plain":"StatementMeta(, 3c166b9a-f167-4f97-8747-8e1fdca2eefa, 30, Finished, Available)"},"metadata":{}}],"execution_count":28,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}}},{"cell_type":"code","source":["#Thanks to MIM for help with this code\r\n","spark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\r\n","spark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\r\n","#Load from the default lakehouse, make sure you click on the pin <=============\r\n","from pyspark.sql.types import *\r\n","df = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/FactSurveyResponse.csv\")\r\n","df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/FactSurveyResponse\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"3c166b9a-f167-4f97-8747-8e1fdca2eefa","statement_id":31,"state":"finished","livy_statement_state":"available","queued_time":"2023-07-01T17:47:30.2561217Z","session_start_time":null,"execution_start_time":"2023-07-01T17:48:51.8739825Z","execution_finish_time":"2023-07-01T17:48:54.5797976Z","spark_jobs":{"numbers":{"RUNNING":0,"FAILED":0,"SUCCEEDED":7,"UNKNOWN":0},"jobs":[{"dataWritten":0,"dataRead":4397,"rowCount":50,"jobId":209,"name":"toString at String.java:2994","description":"Delta: Job group for statement 31:\n#Thanks to MIM for help with this code\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/FactSurveyResponse.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/FactSurveyResponse\"): Compute snapshot for version: 0","submissionTime":"2023-07-01T17:48:53.379GMT","completionTime":"2023-07-01T17:48:53.401GMT","stageIds":[305,303,304],"jobGroup":"31","status":"SUCCEEDED","numTasks":52,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":51,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":2,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":4397,"dataRead":1792,"rowCount":54,"jobId":208,"name":"toString at String.java:2994","description":"Delta: Job group for statement 31:\n#Thanks to MIM for help with this code\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/FactSurveyResponse.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/FactSurveyResponse\"): Compute snapshot for version: 0","submissionTime":"2023-07-01T17:48:53.063GMT","completionTime":"2023-07-01T17:48:53.366GMT","stageIds":[302,301],"jobGroup":"31","status":"SUCCEEDED","numTasks":51,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":1,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":1792,"dataRead":2384,"rowCount":8,"jobId":207,"name":"toString at String.java:2994","description":"Delta: Job group for statement 31:\n#Thanks to MIM for help with this code\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/FactSurveyResponse.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/FactSurveyResponse\"): Compute snapshot for version: 0","submissionTime":"2023-07-01T17:48:52.921GMT","completionTime":"2023-07-01T17:48:52.960GMT","stageIds":[300],"jobGroup":"31","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":0,"rowCount":0,"jobId":206,"name":"","description":"Job group for statement 31:\n#Thanks to MIM for help with this code\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/FactSurveyResponse.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/FactSurveyResponse\")","submissionTime":"2023-07-01T17:48:52.562GMT","completionTime":"2023-07-01T17:48:52.562GMT","stageIds":[],"jobGroup":"31","status":"SUCCEEDED","numTasks":0,"numActiveTasks":0,"numCompletedTasks":0,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":0,"numActiveStages":0,"numCompletedStages":0,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":36700,"dataRead":79111,"rowCount":5454,"jobId":205,"name":"save at <unknown>:0","description":"Job group for statement 31:\n#Thanks to MIM for help with this code\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/FactSurveyResponse.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/FactSurveyResponse\")","submissionTime":"2023-07-01T17:48:52.243GMT","completionTime":"2023-07-01T17:48:52.493GMT","stageIds":[299,298],"jobGroup":"31","status":"SUCCEEDED","numTasks":2,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":1,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":79111,"dataRead":188571,"rowCount":5454,"jobId":204,"name":"save at <unknown>:0","description":"Job group for statement 31:\n#Thanks to MIM for help with this code\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/FactSurveyResponse.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/FactSurveyResponse\")","submissionTime":"2023-07-01T17:48:52.164GMT","completionTime":"2023-07-01T17:48:52.210GMT","stageIds":[297],"jobGroup":"31","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":65536,"rowCount":1,"jobId":203,"name":"load at <unknown>:0","description":"Job group for statement 31:\n#Thanks to MIM for help with this code\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n#Load from the default lakehouse, make sure you click on the pin <=============\nfrom pyspark.sql.types import *\ndf = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/FactSurveyResponse.csv\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/FactSurveyResponse\")","submissionTime":"2023-07-01T17:48:51.943GMT","completionTime":"2023-07-01T17:48:51.991GMT","stageIds":[296],"jobGroup":"31","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}}],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"fec59414-e19d-4b91-9548-387d277047e5"},"text/plain":"StatementMeta(, 3c166b9a-f167-4f97-8747-8e1fdca2eefa, 31, Finished, Available)"},"metadata":{}}],"execution_count":29,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}}}],"metadata":{"language_info":{"name":"python"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"widgets":{},"kernel_info":{"name":"synapse_pyspark"},"save_output":true,"spark_compute":{"compute_id":"/trident/default","session_options":{"enableDebugMode":false,"conf":{}}},"notebook_environment":{},"synapse_widget":{"version":"0.1","state":{}},"trident":{"lakehouse":{"default_lakehouse":"62e855f8-1ce8-4585-9b33-606d5656bd18","known_lakehouses":[{"id":"62e855f8-1ce8-4585-9b33-606d5656bd18"}],"default_lakehouse_name":"AdventureWorksLH","default_lakehouse_workspace_id":"9314a544-8566-4d36-8908-691f48fdf005"}}},"nbformat":4,"nbformat_minor":0}